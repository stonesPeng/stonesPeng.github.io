<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[yarn无法使用在vscode的terminal]]></title>
    <url>%2F2020%2F01%2F08%2Fyarn%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%E5%9C%A8vscode%E7%9A%84terminal%2F</url>
    <content type="text"><![CDATA[yarn无法使用在vscode的terminal问题说明在vscode的terminal中使用yarn命令，会出现： 这个是因为在本地计算机上不能运行您编写的未签名脚本和来自其他用户的签名脚本。 解决办法1.首先以管理员的方式运行Powershell 2.请使用以下命令将计算机上的 执行策略更改为 RemoteSigned 1234#更改执行策略set-ExecutionPolicy RemoteSigned#查看策略get-ExecutionPolicy done.]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows普通用户使用cmd命令]]></title>
    <url>%2F2020%2F01%2F08%2Fwindows%20%E6%99%AE%E9%80%9A%E7%94%A8%E6%88%B7%E4%BD%BF%E7%94%A8cmd%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[windows普通用户使用cmd命令 ping、telnet 都是要打开带管理员权限的命令提示符（cmd）才可以使用的命令，除非您调用管理员账户。 原因是缺少系统环境变量导致的。 1.使用cmd需配置系统环境变量设置的环境path路径，操作步骤如下： 鼠标移到左下角右击后选“系统” 系统属性中选“高级” 在高级中选“环境变量” 进入环境变量后，查看系统变量的path处，将下列代码复制进入粘贴 1%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\; 点确定后，重新尝试即可恢复使用 2.使用telenet要进行额外加载关于telnet，则需要你进行加载后才能使用，步骤如下： 按win+x； 输入appwiz.cpl 点击界面左侧“启动和关闭windows功能” 在windows功能列表中勾选“telnet”客户端 系统正常联网加载后，才能使用telnet功能，其他特殊操作一样，可以参考windows 功能列表]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置多个远程仓库]]></title>
    <url>%2F2020%2F01%2F03%2F%E9%85%8D%E7%BD%AE%E5%A4%9A%E4%B8%AA%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[当一台电脑需要连接多个远程仓库的时候如何配置。如一个仓库需要连接github、另一个仓库需要谅解gitlab、还有一个仓库需要连接gitee等。 同一电脑配置多个仓库，如果仓库不为同一网站则使用同一个公钥即可。本文讲的是分开配置的方法 1.配置一个远程仓库(1)生成ssh-key 输入如下代码生成一个ssh-key ssh-keygen -t rsa -C &quot;yourmail@gmail.com&quot; 一般情况下连续三个回车直接生成ssh-key出现如下方代码 1234567891011121314151617181920Generating public/private ecdsa key pair.Enter file in which to save the key (/home/username/.ssh/id_ecdsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /home/username/.ssh/id_ecdsa.Your public key has been saved in /home/username/.ssh/id_ecdsa.pub.The key fingerprint is:dd:15:ee:24:20:14:11:01:b8:72:a2:0f:99:4c:79:7f username@localhost-2011-12-22The key's randomart image is:+--[ECDSA 521]---+| ..oB=. . || . . . . . || . . . + || oo.o . . = ||o+.+. S . . . ||=. . E || o . || . || |+-----------------+ 第一个回车后会出现 Generating public/private rsa key pair.Enter file in which to save the key (/c/Users/Administrator.2017.V.2.12-318/.ssh/id_rsa): 要求输入想要存储的文件名的位置和名称 （ 如果不输入则名称默认为~/.ssh/id_rsa、如果你仅仅要配置一个帐号，那么我们使用默认名称即可） 第二个回车后出现：输入私钥的密码 Enter passphrase (empty for no passphrase): 第三个回车后：为确认输入私钥的密码 Enter same passphrase again: 这里直接回车默认设置为无密码，使用默认即可 到这里生成SSH-KEY的事就完成了，你在当前文件夹会看到两个文件： id_rsa你的私钥）； id_rsa.pub（你的公钥） (2) 查看的公钥文件的实际内容，添加到对应的 远程仓库账户中1cat ~/.ssh/id_rsa.pub 参数解释 也可以通过如下代码来生成ssh-key ssh-keygen -t rsa -f ~/.ssh/id_rsa_x -C &quot;yourmail@xxx.com&quot; -f 后面内容为指定的ssh-key生成的位置和名称 -t 参数之后，我们请求建立一个 “RSA” 类型的密钥。RSA 是当前最新并且最安全的一种形式。 -C 参数之后，我们提供了一个注释，你可以把它想象为对这个密钥的一种描述或标签。例如使用你的 email 地址。总之，一个能让你之后更容易识别的注释。 2. 配置多个远程仓库生成三个ssh-key 生成三个名称不同ssh-key 分别命名为id_rsa_github；id_rsa_github2；id_rsa_github；id_rsa_gitee 1234# 可以采用如下方式 在第一个回车后输入名称ssh-keygen -t rsa -C "yourmail@gmail.comGenerating public/private rsa key pair.Enter file in which to save the key (/Users/QuQu/.ssh/id_rsa): id_rsa_gitlab 编辑config文件，配置不同的仓库指向不同的密钥文件 如果没有则在~/.ssh文件夹下新建 config文件 windows 在Window进入C:/Users/你的用户名/.ssh文件夹，右键新建一个文本文件，改名为config即可。这里要注意，没有.ssh文件夹的要新建一个.ssh名的文件夹。 Linux进入.ssh文件夹：cd ~/.ssh，新建config文件：touch config；或者：touch ~/.ssh/config。这里要注意，没有.ssh文件夹的要新建一个.ssh名的文件夹。 添加如下内容 12345678910111213141516171819202122232425# github配置Host github.com(可更改)// 主机名字，不能重名 HostName github.com// 主机所在域名或IP User git// 用户名称 PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa_github// 私钥路径# github配置(第二个与第一个不是一个仓库但是同为github)Host github2.com(可更改)// 主机名字，不能重名 HostName github.com// 主机所在域名或IP User git// 用户名称 PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa_github2// 私钥路径# gitlab配置Host gitlib.com(可更改) HostName gitlab.xxx.com(gitlab仓库域名) User git PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa_github# gitee配置Host gitee.com(可更改) HostName gitee.com User git PreferredAuthentications publickey IdentityFile ~/.ssh/id_rsa_github 清空本地的 SSH 缓存，添加新的 SSH 密钥 到 SSH agent中 这里如果你用的github官方的bash，ssh-agent -s,如果是其他的，比如msysgit,eval $(ssh-agent -s) 1234# 如果在.ssh目录下可省略文件路径 :ssh-add id_rsa_githubssh-add -Dssh-add ~/.ssh/id_rsa_githubssh-add ~/.ssh/id_rsa_gitlab 测试 ssh 链接123456ssh -T git@github.comssh -T git@github2.comssh -T git@gitee.comssh -T git@gitlab.com# xxx! You’ve successfully authenticated, but GitHub does not provide bash access.# 出现上述提示，连接成功 取消 git 全局用户名/邮箱的设置，设置独立的 用户名/邮箱123456# 取消全局 用户名/邮箱 配置$ git config --global --unset user.name$ git config --global --unset user.email# 进入项目文件夹，单独设置每个repo 用户名/邮箱$ git config user.email "xxxx@xx.com"$ git config user.name "xxxx" 命令行进入项目目录，重建 origin (whatever 为相应项目地址)123456$ git remote rm origin# 远程仓库地址，注意Host名称$ git remote add origin git@second.github.com:githubUserName/repName.git$ git remote -v # 查看远程# ssh -vT git@github.com可以打印log 通过此方法可以debug如果连接不成功 原理分析 ssh 客户端是通过类似 git@github.com:githubUserName/repName.git 的地址来识别使用本地的哪个私钥的，地址中的 User 是@前面的git， Host 是@后面的github.com。 如果所有账号的 User 和 Host 都为 git 和 github.com，那么就只能使用一个私钥。所以要对User 和 Host 进行配置，让每个账号使用自己的 Host，每个 Host 的域名解析到 github.com，如上面配置中的Host gitlab.com。 配置了别名之后，新的地址就是user@host:repName/repName.git（在添加远程仓库时使用）。 这样 ssh 在连接时就可以区别不同的账号了。 使用新的公私钥 情景1：使用新的公私钥进行克隆操作 12git clone git@gitlab.com:username/repo.git 注意此时要把原来的github.com配置成你定义的github 情景2：已经克隆，之后才添加新的公私钥，我要为仓库设置使用新的公私钥进行push操作 123修改仓库的配置文件：.git/config 为[remote "origin"] url = git@gitlab.com:gitlabUserName/repName.git 转载链接：https://www.jianshu.com/p/93f846cef40d]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sersync&&Rsync]]></title>
    <url>%2F2019%2F11%2F15%2FSersync%26%26Rsync%2F</url>
    <content type="text"><![CDATA[rsync基于SSH协议的一种文件传输应用。 它引入了分片比较传输的方式，因此对于相似文件传输效率很高。 简单说就是，它的算法会高效的分片计算源文件中和目标文件特征码，然后进r行比较，最后仅发送不一致的部分。 rsync会根据文件大小自动判断数据块大小，也可以用命令手动指定，官方建议大小在500-1000字节之间。 对于我们的应用场景，OSS文件同步，文件都是也不可修改，都是全量传输，所以并不用担心效率问题；同时rsync这种方式也非常利于断点续传。 SersyncSersync是金山的工程师基于inotify开发的。 而inotify 是linux的内核的一个用来监控文件系统的变化的模块，它可以监控相应的目录，并把其变化通知到用户空间的应用。但是它的记录只包含了目录发生了变化，并没有把具体是哪个文件或者哪个目录发生了变化记录下来。 Sersync则可以记录下被监听目录中发生变化的（包括增加、删除、修改）具体某一个文件或者某一个目录的名字，而且是持久化的记录，所以可以实现与历史数据的对比。例如开启sersync，然后关闭sersync，修改目录中的文件，再启动sersync。sersync是可以比较出当前目录和历史目录的区别的。 sersync内置了rsync命令调用，从而实现自动同步发生变化的文件或者目录。 不同服务器上目录的双向同步A服务器监控目标目录变化，并将变化通过rsync client命令发到服务器B的rsync server； B服务器监控目标目录变化，并将变化通过rsync client命令发到服务器A的rsync server； 以下未完全验证： A同步变化到B后，B会检查新变化的文件，发送特征到A。A服务器发现特征码完全一致，也就知道B服务器收到了刚才的同步请求。循环的链条就断掉了。]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>sync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx配置单页面应用]]></title>
    <url>%2F2019%2F11%2F13%2FNginx%E9%85%8D%E7%BD%AE%E5%8D%95%E9%A1%B5%E9%9D%A2%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Nginx配置单页面应用1.error起初在nginx.conf静态资源映射的配置如下: 12root /usr/share/nginx/html/appname/;index index.html index.htm; 以上的配置能正常访问index.html,能点击顺序地进入其他页面。但是使用浏览器的browserHistory 和 refresh 页面会报404。 2. reason因为我们部署的是单页面应用,目前主流的前端框架都是单页面的，例如React 和Vue。 单页面简单来说就是访问所有资源路径、其实页面内容只有一个（一般是index.html）。这个页面中引入的js框架会根据当前访问的url去路由到相应的子页面组件（可以理解为页面片段）进行逻辑处理和页面渲染。 所以我们直接访问某个页面资源（例如/usr/info），这个资源是不存在的，所以报服务端就会报404。 3.solution12345root /usr/share/nginx/html/appname/;index index.html index.htm;location / &#123; try_files $uri $uri/ /index.html;&#125; 原理是，当配置try_files找不到某个页面资源，这时，nginx会尝试加载index.html，加载index.html之后，react-router就能起作用并匹配我们输入的/home路由，从而显示正确的home页面。 4.complete完整配置，供参考。 123456789101112131415161718192021222324252627access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; server &#123; listen 80; # gzip config gzip on; gzip_min_length 1k; gzip_comp_level 9; gzip_types text/plain text/css text/javascript application/json application/javascript application/x-javascript application/xml; gzip_vary on; gzip_disable &quot;MSIE [1-6]\.&quot;; root /usr/share/nginx/html/appname/; index index.html index.htm; location / &#123; try_files $uri $uri/ /index.html; &#125; location ^~ /assets/ &#123; gzip_static on; expires max; add_header Cache-Control public; &#125; error_page 500 502 503 504 /500.html; client_max_body_size 20M; keepalive_timeout 10; &#125;]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于GTID的Mysql主从同步]]></title>
    <url>%2F2019%2F11%2F08%2F%E5%9F%BA%E4%BA%8EGTID%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[基于GTID的Mysql主从同步 MySQL是以容易学习和方便的高可用架构，被开发人员青睐的。而它的几乎所有的高可用架构，都直接依赖于binlog。虽然这些高可用架构已经呈现出越来越复杂的趋势，但都是从最基本的一主一备演化过来的。 1.主从同步流程1.1相关概念解释InnoDB 使用undo、 redo log 来保证事务原子性、一致性及持久性，同时采用预写日志（Write-Ahead Logging）方式将随机写入变成顺序追加写入，提升事务性能。 binglog 二进制日志，也称归档日志 binlog日志用于记录所有更新且提交了数据或者已经潜在更新提交了数据的所有数据。用于主从同步和基于时间节点还原。 redolog 重做日志 记录事务将要变更后的状态。事务提交时，只要将redo log 持久化即可，数据可在内存中变更。当系统崩溃时，虽然数据没有落盘，但是redo log 已持久化，系统可以根据redo log 的内容，将所有数据恢复到最新的状态。 undolog 回滚日志 记录事务变更前的状态。操作数据之前，先将数据备份到undo log ，然后进行数据修改，如果出现错误或用户执行了rollback 语句，则系统就可以利用undo log 中的历史版本恢复到事务开始之前的状态。 1.2 流程解析 ​ 主备同步流程图 主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写binlog 备库B跟主库A之间维持了一个长连接。主库A内部有一个线程，专门用于服务备库B的这个长连接。一个事务日志同步的完整过程是这样的： 在备库B上通过change master命令，设置主库A的IP、端口、用户名、密码，以及要从哪个位置开始请求binlog，这个位置包含文件名和日志偏移量。 在备库B上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread和sql_thread。其中io_thread负责与主库建立连接。 主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B。 备库B拿到binlog后，写到本地文件，称为中转日志（relay log）。 sql_thread读取中转日志，解析出日志里的命令，并执行。 Tip:多线程复制方案的引入，sql_thread演化成为了多个线程 2.GTID讲解2.1 GTID是什么GTID的全称是Global Transaction Identifier，也就是全局事务ID，是一个事务在提交的时候生成的，是这个事务的唯一标识。它由两部分组成，格式是： 1GTID=source_id:transaction_id 其中： server_uuid是一个数据库实例第一次启动时自动生成的，是一个全局唯一的值； transaction_id是一个整数，初始值是1，每次提交事务的时候分配给这个事务，并加1。 此时的transaction_id并不是我们平时使用的事务id，事务id是在事务执行过程中分配的，如果这个事务回滚了，事务id也会递增。而此时的transaction_id是在事务提交的时候才会分配，所以GTID往往是连续的。 2.2 GTID的生成方式GTID有两种生成方式，而使用哪种方式取决于session变量gtid_next的值。 在GTID模式下，每个事务都会跟一个GTID一一对应。这个GTID有两种生成方式，而使用哪种方式取决于session变量gtid_next的值。 如果gtid_next=automatic，代表使用默认值。这时，MySQL就会把server_uuid:gno分配给这个事务。a. 记录binlog的时候，先记录一行 SET @@SESSION.GTID_NEXT=‘server_uuid:gno’;b. 把这个GTID加入本实例的GTID集合。 如果gtid_next是一个指定的GTID的值，比如通过set gtid_next=’current_gtid’指定为current_gtid，那么就有两种可能：a. 如果current_gtid已经存在于实例的GTID集合中，接下来执行的这个事务会直接被系统忽略；b. 如果current_gtid没有存在于实例的GTID集合中，就将这个current_gtid分配给接下来要执行的事务，也就是说系统不需要给这个事务生成新的GTID，因此gno也不用加1。 一个current_gtid只能给一个事务使用。这个事务提交后，如果要执行下一个事务，就要执行set 命令，把gtid_next设置成另外一个gtid或者automatic。 每个MySQL实例都维护了一个GTID集合，用来对应“这个实例执行过的所有事务 2.3开启GTID方式在启动数据库实例的时候，配置以下参数即可 12enforce_gtid_consistency=ongtid_mode=on 以上参数缺一不可 3.4 GTID优缺点优点 搭建主从同步便捷 slave 做同步设置时，只需要设置change master to master_auto_position=1即可，无须通过在master执行 show master status找binlog日志名称和POSITION点，MySQL会通过内部机制GTID自动找点同步。 支持多线程复制（基于库） 在Mysql5.6之前，Salve复制是单线程的，一个事件一个事件的读取应用，但客户端并发写入Master时候，会造成从库同步延迟。在MySQL 5.6里面，我们可以把多个表放在多个库，这样就可以使用多线程复制，当只有1个库，多线程复制是没有用的 强大的failover能力（TODO） 当Master crash时，Salve必然面临切换到Master’，GTID提供强大的故障恢复能力 处理主从数据冲突方便 在从库提交空事务的方式进行跳过 缺点 不支持非事务引擎 CREATE TABLE …SELECT，临时表等语句不支持 该语句会被拆分成createtable 和insert两个事务，并且这个两个事务被分配了同一个GTID，这会导致insert被备库忽略掉； 存在Errant transaction 也就是没有规范的从master执行，而是直接从slave执行的事务。需要尽量避免此类事务，如果非要在slave上写数据时，通过关闭logbin解决。如果出现了此类情况，可以根据具体情况执行空事务（重复键值）或者重做Slave（Slave多执行了事务）解决问题。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql主主高可用备份部署例子]]></title>
    <url>%2F2019%2F11%2F07%2FMysql%E4%B8%BB%E4%B8%BB%E9%AB%98%E5%8F%AF%E7%94%A8%E5%A4%87%E4%BB%BD%E9%83%A8%E7%BD%B2%E4%BE%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[Mysql Master-Master Docker 部署例子此教程简要指导Mysql数据库双主备份。 0.环境准备 两台CentOS系统的服务器,服务器分配在同一网段，并保证相互网络畅通 -server-a ip : 192.168.1.220 -server-b ip : 192.168.1.221 服务器预先安装好Docker，Docker-Compose，pull Mysql:8.0镜像 Navicat for mysql 执行数据库命令 1.设置Mysql配置文件新建mysql配置文件my.cnf，详细配置省去，只说明主从备份相关配置。 1.1设置server-a的配置文件与主主备份相关的配置 123456789101112131415161718[mysqld]#为服务器分配id，可以自定义，不区分大小，起标识作用。不同数据库节点分配不同的idserver_id=1# 打开Mysql 日志，日志格式为二进制log-bin=mysql-bin# 可选项Mixed,Statement,Row，默认格式是 Statement，mixed混合Satement，ROW两种模式binlog_format=mixed#当启用时，服务器通过只允许执行可以使用GTID安全地记录的语句来强制GTID一致性。enforce-gtid-consistency=true#启用基于GTID的复制，启用之前必须保证enforce-gtid-consistency=truegtid_mode=ON#该选项让从库写入哪些来自于主库的更新，并把这些更新写入bin-log文件，一台服务器即做主库又做从库必须开启log-slave-updates=true 一些定制化的配置 针对主服务器 忽略不同步主从的数据库 12#一般设置 sys,performace_schema,infomation_schema,mysqlbinlog-ignore-db=&lt;YOUR-DB-NAME&gt; 允许同步的数据库 12#一般设置成备份的生产数据的数据库binlog-do-db=&lt;YOUR-DB-NAME&gt; 针对从服务器 忽略不同步主从的数据库 12#一般设置 sys,performace_schema,infomation_schema,mysqlreplicate-ignore-db=&lt;YOUR-DB-NAME&gt; 允许同步的数据 12#一般设置成备份的生产数据的数据库replicate-do-db=&lt;YOUR-DB-NAME&gt; 因为我们配置的是双主服务器，每台服务器既要作为主又要作为从。所以以上针对主，从服务器的个性化配置在两台数据库都要按具体业务需求进行配置。 另，以上4个定制化参数可重复设置，如下图 12replicate-ignore-db=sysreplicate-ignore-db=mysql 1.2 设置server-b配置区别于server-a的配置，改动以下配置即可,其余定制化配置按需自定义 12#server_id 必须独一无二server_id=2 2.编辑docker-compose.yml两个服务器配置相同内容的docker-compoe.yml文件 1234567891011121314151617version: &apos;2.2&apos;services: #mysql service mysql: image: mysql:8.0 ports: - &quot;3306:3306&quot; command: --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci --default-time-zone=&apos;+8:00&apos; restart: always volumes: #持久化数据库，注意路径 - ./data/db:/var/lib/mysql #替换数据库配置文件，注意路径 - ./my.conf:/etc/mysql/conf.d/mysql.cnf environment: MYSQL_ROOT_PASSWORD: &lt;YOUR_PAASWORD&gt; container_name: mysql_service 3.docker 开启数据库容器每个服务器针对数据库的docker-compose目录如下（供参考）： -mysql_replicate docker-compose.yml my.conf -data -db 启动数据库服务 1docker-compose up -d 确保数据库服务启动成功 4.配置同步信息并启动同步4.1 查看两台服务器Mysql状态1234567MySQL [(none)]&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000014 | 81 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) File 表示当前正在写入的binlog文件 Postion 表示当前正在写入的位置 Binlog_Do_DB表示只记录指定数据库的二进制文件 Binlog_Ignore_DB表示不记录指定数据库的二进制文件 Executed_Gtid_Set这是5.7mysql库引入了新的表gtid_executed，会记录当前执行的GTID 如果未开启GTID，配置同步信息会用到File和Postion 4.2创建Replication用户创建针对限定host的Slave登录用账户和密码，并刷新权限 针对server-a,其ip为192.168.8.220 123create user &apos;repl&apos;@&apos;192.168.1.221&apos; identified by &apos;vo7kphndxlzfr6u3&apos;;grant replication slave on *.* to &apos;repl&apos;@&apos;192.168.1.221&apos;;flush privileges; 针对server-b,其ip为192.168.8.221 123create user &apos;repl&apos;@&apos;192.168.1.220&apos; identified by &apos;vo7kphndxlzfr6u3&apos;;grant replication slave on *.* to &apos;repl&apos;@&apos;192.168.1.220&apos;;flush privileges; 4.3配置同步信息使数据库实例和Master实例关联起来 针对server-a,其ip为192.168.8.220 1234567change master to master_host=&apos;192.168.1.221&apos;, master_port=3306, master_user=&apos;repl&apos;, master_password=&apos;vo7kphndxlzfr6u3&apos;,MASTER_AUTO_POSITION=1,GET_MASTER_PUBLIC_KEY=1; 针对server-b,其ip为192.168.8.221 1234567change master to master_host=&apos;192.168.1.220&apos;, master_port=3306, master_user=&apos;repl&apos;, master_password=&apos;vo7kphndxlzfr6u3&apos;,MASTER_AUTO_POSITION=1,GET_MASTER_PUBLIC_KEY=1; MASTER_AUTO_POSITION自动获取Master 的位置，就不要去指定master_log_file和master_log_pos。 GET_MASTER_PUBLIC_KEY很重要，如果不设置此参数，会导致Slave开启后，Slave无法连接到Master。错误见下图 12eror connecting to master &apos;repl@mysql-master:3306&apos; - retry-time: 60 retries: 3, Error_code: MY-002061... 这个是因MySQL 8默认启用了caching_sha2_password authentication plugin，通过mysql官方文档介绍可以在CHANGE MASTER TO添加GET_MASTER_PUBLIC_KEY=1参数来解决这个问题。 4.4 启动同步和查看同步状态在两个数据库实例中启动同步 1start slave; 表示启动从库的两个线程： I/O线程，表示线程会连接远程Master的mysql，并把它的bin-log拷贝到本机的中继日志relay-log中。 sql线程，表示该线程会读取本机中继日志中的数据，并把这些数据恢复到本机自己的mysql表中。 这样Master的数据就会同步到从库的数据库中。 查看同步状态 1show slave status; 显示以下状态则正常 12Slave_IO_Running: YesSlave_SQL_Running: Yes 至此，配置同步信息，启动同步完成。修改任意设置的数据库表结构，库结构，会同步更新到另一个数据库中。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正向代理和反向代理]]></title>
    <url>%2F2019%2F11%2F05%2F%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86%E5%92%8C%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[图解正向代理和反向代理]]></content>
      <categories>
        <category>部署运维</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install docker ,docker-compose in centos 7]]></title>
    <url>%2F2019%2F10%2F30%2Finstall-docker-in-centos%2F</url>
    <content type="text"><![CDATA[安装Docker，Docker-Compose in centos 7本教程指导如何安装Docker CE ，Docker-compose 在Centos 7 上 1.安装Docker CEDocker CE 分为 stable test 和 nightly 三个更新频道。每六个月发布一个 stable 版本 (18.09, 19.03, 19.09…)，生产环境使用stable版本。 1.1 卸载旧版本Docker,Docker-Engine确定未安装过Docker可不进行此步骤 12345678910$ sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 1.2 配置 Ali Docker YUM源12#first 安装依赖包$sudo yum install -y yum-utils device-mapper-persistent-data lvm2 12#then 使用ali仓库源$sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 1.3安装Docker CE查看可供安装的版本 12#按需选择适合自己的版本$yum list docker-ce --showduplicates 非脚本安装latest stable 123#更新源缓存$sudo yum makecache fast$sudo yum install docker-ce 脚本安装latest stable 仅限于centos上使用此方法，简化安装步骤，使用–mirror 设定国内源，加快下载速度 12$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun 若是需要旧版本的Docker ce，可按照以下方法安装 1234#安装较旧版本，需要指定完整的rpm包的包名，并加上参数--setopt=obsoletes=0$yum install -y --setopt=obsoletes=0 \ docker-ce-17.03.3.ce-1.el7.centos.x86_64 \ docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch 12#安装较新的版本，加上rpm包名的版本号部分即可$sudo yum install docker-ce-18.03.0.ce 1.4启动Docker 服务1234# 配置开机启动$ sudo systemctl enable docker# 启动docker 服务$ sudo systemctl start docker 使用Docker –version Docker 安装启动成功 2.设置Docker 国内镜像源使用 Docker 需要经常从官方获取镜像，国内拉取镜像的过程非常耗时，所以要更换到国内镜像源 a.创建或修改 /etc/docker/daemon.json 1234# 设置网易镜像源，也可选择其它&#123; &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;]&#125; b.重启Docker服务，done 1$systemctl restart docker 若不想设置全局的镜像源，也可在拉取镜像时指定镜像源 12#临时指定镜像源$docker pull registry.docker-cn.com/library/ubuntu:16.04 3.安装Docker-Compose安装Docker CE 并不会安装Docker-Compose，需要另外安装 a.使用Curl下载Docker-Compose 的二进制文件到/usr/local/bin目录 1$sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.23.1/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose 因为网络状况，下载过程也许会很缓慢，耐心等待… b.使二进制文件可执行 1$sudo chmod +x /usr/local/bin/docker-compose c.验证是否成功 1$docker-compose --version]]></content>
      <categories>
        <category>部署运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高可用服务集群-先导篇]]></title>
    <url>%2F2019%2F10%2F21%2F%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9C%8D%E5%8A%A1%E9%9B%86%E7%BE%A4-%E5%BC%95%E5%AF%BC%E7%AF%87%2F</url>
    <content type="text"><![CDATA[高可用服务集群一个高可用的服务意味着两点： 第一点，该服务不能频繁出现故障，出现任何的故障都会对业务产生或大或小的影响 第二点，即使服务出现故障，也应该要在极短的时间内恢复正常到业务，将损失降到最低 1.衡量高可用服务的重要指标 业界一般以“提供正常服务的时间/全年的时间”取百分比作为衡量一个服务高可用程度的重要指标，也就是我们经常听到的N个9的说法。比如4个9，也就是99.99%。小数点后9越多代表服务更具备高可用性。 在生产环境中，我们并不一定非要追求的极致高可用，我们应该根据能够承担多少宕机成本，就保证相应的可用时间这样的原则进行高可用的时间。这个原则我们可以从经济效益去理解，如果你为实现更高可用的成本大于实现后带来的利益，你就应该斟酌一番了。 2.如何提高服务的高可用性提高的方式多种多样，但总的来说我们需要从两方面着手。 从平均失效时间入手 从平均恢复时间入手 首先，可以尽量避免应用宕机来减少宕机时间。实际上，通过适当的配置、监控，以及规范或安全保障措施，很多导致宕机的问题很容易可以避免。 其次，尽量保证在发生宕机时，能够快速恢复。最常见的策略是在系统中制造冗余，并且保证系统的故障转移能力。 具体方法有： 基于负载均衡的故障转移 冗余备份 超时设置 异步调用 服务分级和降级 监控告警 防雪崩机制 流量缓冲机制 自动化测试 灰度发布和回滚机制 代码控制 接下来的文章结合Keepalived展开聊一聊故障转移中的虚拟 IP 地址或 IP 接管方案，其余方案后续一一详细阐述。]]></content>
      <categories>
        <category>部署运维</category>
      </categories>
      <tags>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn换镜像源]]></title>
    <url>%2F2019%2F09%2F06%2Fyarn%E6%8D%A2%E9%95%9C%E5%83%8F%E6%BA%90%2F</url>
    <content type="text"><![CDATA[yarn换镜像源 yarn或者npm默认配置的数据源在国内开发环境下，安装包的速度可能比较慢，影响开发效率，可以切换为国内的taobao源，大大地提升下载包速度。 1.查看当前源配置地址12yarn config get registry# -&gt;https://registry.yarnpkg.com 2.设置taobao镜像源1yarn config set registry &apos;https://registry.npm.taobao.org&apos; -设置后执行结果- 设置成功，用yarn install即可享受更快速的下载速度，npm换源同理 6.部分源地址12345678910111213npm --- https://registry.npmjs.org/cnpm --- https://r.cnpmjs.org/taobao --- https://registry.npm.taobao.org/nj --- https://registry.nodejitsu.com/rednpm --- https://registry.mirror.cqupt.edu.cn/npmMirror --- https://skimdb.npmjs.com/registry/deunpm --- http://registry.enpmjs.org/]]></content>
      <categories>
        <category>部署运维</category>
      </categories>
      <tags>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AutoCloseable & Closable]]></title>
    <url>%2F2019%2F09%2F04%2FAutoCloseable%2F</url>
    <content type="text"><![CDATA[AutoCloseable &amp; ClosableAutoCloseable &amp; Closable 是 Java IO流中的两个接口，后者继承自前者。都只有一个 close 方法。 Syntactic sugar AutoCloseable 出现在 Java7 中，它的作用是：减少代码的繁冗度，对于需要关闭的资源使用一种“try-with-resource”方式关闭，这是一个语法糖，即便于程序员编写代码的一种特制语法。 AutoCloseable 是为了支持 try-with-resource 语法出现的工具，实现这个接口的资源类意味着该类可以使用 try-with-resource，否则不能使用。 Who call close? 实际上这个语法糖在代码编译时会被编译为“经典写法”，可以使用如下的代码验证最终 close 方法的调用者是资源类使用所在地。 What else? AutoCloseable 是一个更高抽象层次的接口，不一定用在 IO 上，所以它里面的 close 方法抛出的异常是 Exception，而非 IOException。 Closable 是一个被具现化了的接口：只用作 IO 流上，所以 close 方法抛出的异常是 IOException。 最后一个区别是，Closeable 方法被设计为可重复调用：可以被自动调用：故 Closeable 继承了 AutoCloseable，也可以被手动调用，所以实现了 Closeable 的类的 close 方法必须要被设计为可重复被多次调用。而 AutoCloseabe 不具体这个性质，它就是被设计为“自动关闭”的，所以实现这个接口的类的 close 方法可以不被设计为“可被多次调用”。 &gt;原文链接]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PC查看WiFi的连接密码]]></title>
    <url>%2F2019%2F08%2F30%2FPC%E6%9F%A5%E7%9C%8BWiFi%E7%9A%84%E8%BF%9E%E6%8E%A5%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[查看笔记连接过WiFi的密码操作方法1.以管理员身份进入命令行页面 2.进入netsh 3.查看WLAN配置文件信息 4.获取指定连接安全密钥 12341234即WiFi连接密码，done…]]></content>
      <categories>
        <category>部署运维</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程-并发编程工具类]]></title>
    <url>%2F2019%2F08%2F26%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[并发编程工具类大纲]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(三)python-高级特性]]></title>
    <url>%2F2019%2F08%2F19%2Fpython-%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[学习大纲]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(二)python-函数]]></title>
    <url>%2F2019%2F08%2F19%2Fpython-%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[学习大纲]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(一) python-基础]]></title>
    <url>%2F2019%2F08%2F19%2Fpython-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[学习大纲]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot with docker]]></title>
    <url>%2F2019%2F07%2F31%2Fspringbootdocker%2F</url>
    <content type="text"><![CDATA[Springboot with docker0.准备工程 在pom.xml增加docker构建插件 12345678910111213141516plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/imageName&gt; &lt;dockerDirectory&gt;src/main/docker&lt;/dockerDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt;&lt;/plugin&gt; 在工程src/main/docker目录下添加DockerFile文件 1234FROM openjdk:8-jdk-alpineVOLUME /tmpADD webme-0.0.1-SNAPSHOT.jar app.jarENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] 测试项目是否能正常运行 1mvn package -Dmaven.test.skip=true #打包项目 1java -Djava.security.egd=file:/dev/./urandom -jar /app.jar #运行项目 1.maven使用DockerFile构建镜像1mvn package docker:build #首次构建需要时间 构建好的镜像通过docker images查看 2.运行镜像1234docker run -p 8001:8080 -dti &lt;imagesName&gt; /bin/bash #启动一个 bash 终端，允许用户进行交互#-t 让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上#-i 让容器的标准输入保持打开#-d 让Docker容器在后台以守护态（Daemonized）形式运行 启动成功使用docker ps 查看运行的容器 3.删除镜像/容器12docker rm &lt;contaninerId&gt;/&lt;containerName&gt; #删除容器，一般删除之前要通过stop中断容器docker rmi &lt;imageId&gt;/&lt;imageName&gt; #删除镜像 4.导出/导入容器12docker export &lt;containerId&gt; &gt;**.tar #导出容器快照到本地文件cat **.tar | docker import - &lt;imagesName&gt;:&lt;version&gt; # 从容器快照文件中再导入为镜像 Tip 运行导入的镜像必须带command，否则启动会报错：Error response from daemon: No command specified 必须像下列输入命令 1docker run -d -p 9999:9093 sso:latest java -Djava.security.egd=file:/dev/./urandom -jar /app.jar github源代码]]></content>
      <categories>
        <category>spring全家桶</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看并中断端口占用]]></title>
    <url>%2F2019%2F07%2F29%2F%E6%9F%A5%E7%9C%8B%E5%B9%B6%E4%B8%AD%E6%96%AD%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%2F</url>
    <content type="text"><![CDATA[查看端口/进程占用情况，终止进程1.lsof(list open files) 列出当前系统打开文件的工具,通过lsof工具能够查看应用程序打开文件的描述符列表，能对系统监测以及排错提供相当大的帮助。 1lsof -i :portNumber 列出谁在使用某个端口 2.netstat netstate 命令用于显示各种网络相关信息，如网络连接，路由表，接口状态 (Interface Statistics)，masquerade 连接，多播成员 (Multicast Memberships) 等等。 常见参数 参数 说明 -a (all) 显示所有选项，默认不显示LISTEN相关 -t (tcp) 仅显示tcp相关选项 -u (udp) 仅显示udp相关选项 -n 拒绝显示别名，能显示数字的全部转化成数字 -l 拒绝显示别名，能显示数字的全部转化成数字 -p 显示建立相关链接的程序名 -r 显示路由信息，路由表 -e 显示扩展信息，例如uid等 -s 按各个协议进行统计 -c 每隔一个固定时间，执行该netstat命令 1netstat -anp |grep portNumber 查看占用某个端口的进程 3.ps ps命令用于报告当前系统的进程状态。可以搭配kill指令随时中断、删除不必要的程序。非常强大的进程查看命令，使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等，总之大部分信息都是可以通过执行该命令得到的 1ps -ef |grep ssh 4.kill常见参数 12345-a：当处理当前进程时，不限制命令名和进程号的对应关系；-l &lt;信息编号&gt;：若不加&lt;信息编号&gt;选项，则-l参数会列出全部的信息名称；-p：指定kill 命令只打印相关进程的进程号，而不发送任何信号；-s &lt;信息名称或编号&gt;：指定要送出的信息；-u：指定用户。 12kill -l 列出所有信号名称，只有第9种信号(SIGKILL)才可以无条件终止进程kill -9 pid 彻底杀死指定进程，init进程不可杀死，其它所有进程都是init进程的子孙]]></content>
      <categories>
        <category>部署运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git命令大全]]></title>
    <url>%2F2019%2F07%2F25%2FGit%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[git init # 初始化本地git仓库（创建新仓库）git config –global user.name “xxx” # 配置用户名git config –global user.email “xxx@xxx.com“ # 配置邮件git config –global color.ui true # git status等命令自动着色git config –global color.status autogit config –global color.diff autogit config –global color.branch autogit config –global color.interactive autogit config –global –unset http.proxy # remove proxy configuration on gitgit clone git+ssh://git@192.168.53.168/VT.git # clone远程仓库git status # 查看当前版本状态（是否修改）git add xyz # 添加xyz文件至indexgit add . # 增加当前子目录下所有更改过的文件至indexgit commit -m ‘xxx’ # 提交git commit –amend -m ‘xxx’ # 合并上一次提交（用于反复修改）git commit -am ‘xxx’ # 将add和commit合为一步git rm xxx # 删除index中的文件git rm -r * # 递归删除git log # 显示提交日志git log -1 # 显示1行日志 -n为n行git log -5git log –stat # 显示提交日志及相关变动文件git log -p -mgit show dfb02e6e4f2f7b573337763e5c0013802e392818 # 显示某个提交的详细内容git show dfb02 # 可只用commitid的前几位git show HEAD # 显示HEAD提交日志git show HEAD^ # 显示HEAD的父（上一个版本）的提交日志 ^^为上两个版本 ^5为上5个版本git tag # 显示已存在的taggit tag -a v2.0 -m ‘xxx’ # 增加v2.0的taggit show v2.0 # 显示v2.0的日志及详细内容git log v2.0 # 显示v2.0的日志git diff # 显示所有未添加至index的变更git diff –cached # 显示所有已添加index但还未commit的变更git diff HEAD^ # 比较与上一个版本的差异git diff HEAD – ./lib # 比较与HEAD版本lib目录的差异git diff origin/master..master # 比较远程分支master上有本地分支master上没有的git diff origin/master..master –stat # 只显示差异的文件，不显示具体内容git remote add origin git+ssh://git@192.168.53.168/VT.git # 增加远程定义（用于push/pull/fetch）git branch # 显示本地分支git branch –contains 50089 # 显示包含提交50089的分支git branch -a # 显示所有分支git branch -r # 显示所有原创分支git branch –merged # 显示所有已合并到当前分支的分支git branch –no-merged # 显示所有未合并到当前分支的分支git branch -m master master_copy # 本地分支改名git checkout -b master_copy # 从当前分支创建新分支master_copy并检出git checkout -b master master_copy # 上面的完整版git checkout features/performance # 检出已存在的features/performance分支git checkout –track hotfixes/BJVEP933 # 检出远程分支hotfixes/BJVEP933并创建本地跟踪分支git checkout v2.0 # 检出版本v2.0git checkout -b devel origin/develop # 从远程分支develop创建新本地分支devel并检出git checkout – README # 检出head版本的README文件（可用于修改错误回退）git merge origin/master # 合并远程master分支至当前分支git cherry-pick ff44785404a8e # 合并提交ff44785404a8e的修改git push origin master # 将当前分支push到远程master分支git push origin :hotfixes/BJVEP933 # 删除远程仓库的hotfixes/BJVEP933分支git push –tags # 把所有tag推送到远程仓库git fetch # 获取所有远程分支（不更新本地分支，另需merge）git fetch –prune # 获取所有原创分支并清除服务器上已删掉的分支git pull origin master # 获取远程分支master并merge到当前分支git mv README README2 # 重命名文件README为README2git reset –hard HEAD # 将当前版本重置为HEAD（通常用于merge失败回退）git rebasegit branch -d hotfixes/BJVEP933 # 删除分支hotfixes/BJVEP933（本分支修改已合并到其他分支）git branch -D hotfixes/BJVEP933 # 强制删除分支hotfixes/BJVEP933git ls-files # 列出git index包含的文件git show-branch # 图示当前分支历史git show-branch –all # 图示所有分支历史git whatchanged # 显示提交历史对应的文件修改git revert dfb02e6e4f2f7b573337763e5c0013802e392818 # 撤销提交dfb02e6e4f2f7b573337763e5c0013802e392818git ls-tree HEAD # 内部命令：显示某个git对象git rev-parse v2.0 # 内部命令：显示某个ref对于的SHA1 HASHgit reflog # 显示所有提交，包括孤立节点git show HEAD@{5}git show master@{yesterday} # 显示master分支昨天的状态git log –pretty=format:’%h %s’ –graph # 图示提交日志git show HEAD~3git show -s –pretty=raw 2be7fcb476git stash # 暂存当前修改，将所有至为HEAD状态git stash list # 查看所有暂存git stash show -p stash@{0} # 参考第一次暂存git stash apply stash@{0} # 应用第一次暂存git grep “delete from” # 文件中搜索文本“delete from”git grep -e ‘#define’ –and -e SORT_DIRENTgit gcgit fsck]]></content>
      <categories>
        <category>部署运维</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Cache Key]]></title>
    <url>%2F2019%2F07%2F25%2FSpring-Cache-Key%2F</url>
    <content type="text"><![CDATA[Spring Cache 设置Key 为了提升项目的并发性能，考虑引入本地内存Cache，对：外部数据源访问、Restful API调用、可重用的复杂计算 等3种类型的函数处理结果进行缓存。目前采用的是spring Cache的@Cacheable注解方式，缓存具体实现选取的是Guava Cache。具体缓存的配置此处不再介绍，重点对于key的配置进行说明： 1、基本形式@Cacheable(value=”cacheName”, key”#id”)public ResultDTO method(int id);2、组合形式@Cacheable(value=”cacheName”, key”T(String).valueOf(#name).concat(‘-‘).concat(#password))public ResultDTO method(int name, String password);3、对象形式@Cacheable(value=”cacheName”, key”#user.id)public ResultDTO method(User user);4、自定义key生成器@Cacheable(value=”gomeo2oCache”, keyGenerator = “keyGenerator”)public ResultDTO method(User user);注意：Spring默认的SimpleKeyGenerator是不会将函数名组合进key中的如下：@Componentpublic class CacheTestImpl implements CacheTest { @Cacheable(“databaseCache”) public Long test1() { return 1L; } 1234567891011@Cacheable(&quot;databaseCache&quot;)public Long test2()&#123; return 2L; &#125; @Cacheable(&quot;databaseCache&quot;)public Long test3()&#123; return 3L; &#125; @Cacheable(&quot;databaseCache&quot;)public String test4()&#123; return &quot;4&quot;; &#125; }我们期望输出：1234实际却输出：111ClassCastException: java.lang.Long cannot be cast to java.lang.String此外，原子类型的数组，直接作为key使用也是不会生效的为了解决上述2个问题，自定义了一个KeyGenerator如下：class CacheKeyGenerator implements KeyGenerator { // custom cache key public static final int NO_PARAM_KEY = 0; public static final int NULL_PARAM_KEY = 53; 123456789101112131415161718192021222324252627282930313233@Overridepublic Object generate(Object target, Method method, Object... params) &#123; StringBuilder key = new StringBuilder(); key.append(target.getClass().getSimpleName()).append(&quot;.&quot;).append(method.getName()).append(&quot;:&quot;); if (params.length == 0) &#123; return key.append(NO_PARAM_KEY).toString(); &#125; for (Object param : params) &#123; if (param == null) &#123; log.warn(&quot;input null param for Spring cache, use default key=&#123;&#125;&quot;, NULL_PARAM_KEY); key.append(NULL_PARAM_KEY); &#125; else if (ClassUtils.isPrimitiveArray(param.getClass())) &#123; int length = Array.getLength(param); for (int i = 0; i &lt; length; i++) &#123; key.append(Array.get(param, i)); key.append(&apos;,&apos;); &#125; &#125; else if (ClassUtils.isPrimitiveOrWrapper(param.getClass()) || param instanceof String) &#123; key.append(param); &#125; else &#123; log.warn(&quot;Using an object as a cache key may lead to unexpected results. &quot; + &quot;Either use @Cacheable(key=..) or implement CacheKey. Method is &quot; + target.getClass() + &quot;#&quot; + method.getName()); key.append(param.hashCode()); &#125; key.append(&apos;-&apos;); &#125; String finalKey = key.toString(); long cacheKeyHash = Hashing.murmur3_128().hashString(finalKey, Charset.defaultCharset()).asLong(); log.debug(&quot;using cache key=&#123;&#125; hashCode=&#123;&#125;&quot;, finalKey, cacheKeyHash); return key.toString();&#125; } 采用此方式后可以解决：多参数、原子类型数组、方法名识别 等问题]]></content>
      <categories>
        <category>spring全家桶</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解释和编译语言]]></title>
    <url>%2F2019%2F07%2F22%2F%E8%A7%A3%E9%87%8A%E5%92%8C%E7%BC%96%E8%AF%91%E8%AF%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[常见的编译性语言和解释性语言解释性语言：Java、Python、Perl、C#、JavaScript、VBScript、Ruby、MATLAB等。编译性语言：C/C++、Pascal/Object Pascal（Delphi）等。 计算机不能直接理解高级语言，只能直接理解机器语言，所以必须要把高级语言翻译成机器语言，计算机才能执行高级语言编写的程序。 机器翻译的方式有两种，一个是编译，一个是解释。两种方式只是翻译的时间不同。 编译性语言编译型语言写的程序执行之前，需要一个专门的编译过程，把程序编译成为机器语言的文件，比如exe文件，以后要运行的话就不用重新翻译了，直接使用编译的结果就行了（exe文件），因为翻译只做了一次，运行时不需要翻译，所以编译型语言的程序执行效率高。 解释性语言解释则不同，解释性语言的程序不需要编译，省了道工序，解释性语言在运行程序的时候才翻译，比如解释性java语言，专门有一个解释器能够直接执行java程序，每个语句都是执行的时候才翻译。这样解释性语言每执行一次就要翻译一次，效率比较低。 脚本语言脚本语言是解释性语言。脚本语言一般都有相应的脚本引擎来解释执行。它们一般需要解释器才能运行。所以只要系统上有相应语言的解释程序就可以做到跨平台。脚本语言是一种解释性的语言，例如vbscript,javascript,install shield script等等,它不象c\c++等可以编译成二进制代码，以可执行文件的形式存在。脚本语言不需要编译，可以直接用，由解释器来负责解释。 JAVA语言java语言是特殊的解释性语言。java程序同样需要编译，但是没有直接编译称为机器语言，而是编译为字节码，然后用解释方式执行字节码。Java既可以被编译，也可以被解释。通过编译器，可以把Java程序翻译成一种中间代码 - 称为字节码 - 可以被Java解释器解释的独立于平台的代码。通过解释器，每条Java字节指令被分析，然后在计算机上运行。只需编译一次，程序运行时解释执行，实现跨平台功能。 Java字节码使“写一次，到处运行”成为可能。可以在任何有Java编译器的平台上把Java程序编译成字节码。这个字节码可以运行在任何JVM(Java虚拟机)上。例如，同一个Java程序可以运行在WindowsNT、Solaris和Macintosh上。 编译器与解释器的区别编译型语言在程序执行之前，有一个单独编译过程，将程序翻译成机器语言，以后执行这个程序时，就不用再进行翻译了。 解释型语言，是在运行的时候将程序翻译成机器语言，所以运行速度相对于编译型语言要慢。 编译型与解释型，两者各有利弊。前者由于程序执行速度快，同等条件下对系统要求较低，因此像开发操作系统、大型应用程序、数据库系统等时都采用它，像C/C++、Pascal/Object Pascal（Delphi）等都是编译语言，而一些网页脚本、服务器脚本及辅助开发接口这样的对速度要求不高、对不同系统平台间的兼容性有一定要求的程序则通常使用解释性语言，如Java、JavaScript、VBScript、Perl、Python、Ruby、MATLAB 等等。 编译性语言不如解释性语言跨平台性好？解释性语言，例如java语言，java程序首先通过编译器编译成.class文件，如果在windows平台上运行，则通过windows平台上的java虚拟机（VM）进行解释。如果运行在linux平台上，则通过linux平台上的java虚拟机进行解释执行。所以说能跨平台，前提是平台上必须要有相匹配的java虚拟机。如果没有java虚拟机，则不能进行跨平台。]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lombok安装和注解说明]]></title>
    <url>%2F2019%2F07%2F02%2Flombok%E5%AE%89%E8%A3%85%E5%92%8C%E6%B3%A8%E8%A7%A3%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[Pom依赖12345678&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.8&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Idea安装插件 Lombok注解说明val：用在局部变量前面，相当于将变量声明为final @NonNull：给方法参数增加这个注解会自动在方法内对该参数进行是否为空的校验，如果为空，则抛出NPE（NullPointerException） @Cleanup：自动管理资源，用在局部变量之前，在当前变量范围内即将执行完毕退出之前会自动清理资源，自动生成try-finally这样的代码来关闭流 @Getter/@Setter：用在属性上，再也不用自己手写setter和getter方法了，还可以指定访问范围 @ToString：用在类上，可以自动覆写toString方法，当然还可以加其他参数，例如@ToString(exclude=”id”)排除id属性，或者@ToString(callSuper=true, includeFieldNames=true)调用父类的toString方法，包含所有属性 @EqualsAndHashCode：用在类上，自动生成equals方法和hashCode方法 @NoArgsConstructor, @RequiredArgsConstructor and @AllArgsConstructor：用在类上，自动生成无参构造和使用所有参数的构造函数以及把所有@NonNull属性作为参数的构造函数，如果指定staticName = “of”参数，同时还会生成一个返回类对象的静态工厂方法，比使用构造函数方便很多 @Data：注解在类上，相当于同时使用了@ToString、@EqualsAndHashCode、@Getter、@Setter和@RequiredArgsConstrutor这些注解，对于POJO类十分有用 @Value：用在类上，是@Data的不可变形式，相当于为属性添加final声明，只提供getter方法，而不提供setter方法 @Builder：用在类、构造器、方法上，为你提供复杂的builder APIs，让你可以像如下方式一样调用Person.builder().name(&quot;Adam Savage&quot;).city(&quot;San Francisco&quot;).job(&quot;Mythbusters&quot;).job(&quot;Unchained Reaction&quot;).build();更多说明参考Builder @SneakyThrows：自动抛受检异常，而无需显式在方法上使用throws语句 @Synchronized：用在方法上，将方法声明为同步的，并自动加锁，而锁对象是一个私有的属性$lock或$LOCK，而java中的synchronized关键字锁对象是this，锁在this或者自己的类对象上存在副作用，就是你不能阻止非受控代码去锁this或者类对象，这可能会导致竞争条件或者其它线程错误 @Getter(lazy=true)：可以替代经典的Double Check Lock样板代码 1@Log ：根据不同的注解生成不同类型的log对象，但是实例名称都是log，有六种可选实现类 @CommonsLog Creates log = org.apache.commons.logging.LogFactory.getLog(LogExample.class); @Log Creates log = java.util.logging.Logger.getLogger(LogExample.class.getName()); @Log4j Creates log = org.apache.log4j.Logger.getLogger(LogExample.class); @Log4j2 Creates log = org.apache.logging.log4j.LogManager.getLogger(LogExample.class); @Slf4j Creates log = org.slf4j.LoggerFactory.getLogger(LogExample.class); @XSlf4j Creates log = org.slf4j.ext.XLoggerFactory.getXLogger(LogExample.class);]]></content>
      <categories>
        <category>spring全家桶</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java实体映射工具Mapstruct]]></title>
    <url>%2F2019%2F07%2F01%2Fjava%E5%AE%9E%E4%BD%93%E6%98%A0%E5%B0%84%E5%B7%A5%E5%85%B7Mapstruct%2F</url>
    <content type="text"><![CDATA[Java 实体映射工具 MapStruct原文链接：www.jianshu.com 123声明：1、DO（业务实体对象），DTO（数据传输对象）。2、我的代码中用到了 Lombok ，不了解的可以自行了解一下，了解的忽略这条就好。 在一个成熟的工程中，尤其是现在的分布式系统中，应用与应用之间，还有单独的应用细分模块之后，DO 一般不会让外部依赖，这时候需要在提供对外接口的模块里放 DTO 用于对象传输，也即是 DO 对象对内，DTO对象对外，DTO 可以根据业务需要变更，并不需要映射 DO 的全部属性。 这种 对象与对象之间的互相转换，就需要有一个专门用来解决转换问题的工具，毕竟每一个字段都 get/set 会很麻烦。 MapStruct 就是这样的一个属性映射工具，只需要定义一个 Mapper 接口，MapStruct 就会自动实现这个映射接口，避免了复杂繁琐的映射实现。MapStruct官网地址： mapstruct.org/ 工程中引入 maven 依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;properties&gt; &lt;mapstruct.version&gt;1.3.0.Final&lt;/mapstruct.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct&lt;/artifactId&gt; &lt;version&gt;$&#123;org.mapstruct.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;!-- or newer version --&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;!-- depending on your project --&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;!-- depending on your project --&gt; &lt;annotationProcessorPaths&gt; &lt;path&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;$&#123;org.mapstruct.version&#125;&lt;/version&gt; &lt;/path&gt; &lt;path&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;$&#123;lombok.version&#125;&lt;/version&gt; &lt;/path&gt; &lt;!-- other annotation processors --&gt; &lt;/annotationProcessorPaths&gt; &lt;compilerArgs&gt; &lt;compilerArg&gt; -Amapstruct.suppressGeneratorTimestamp=true &lt;/compilerArg&gt; &lt;compilerArg&gt; -Amapstruct.suppressGeneratorVersionInfoComment=true &lt;/compilerArg&gt; &lt;/compilerArgs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 基本映射这里定义两个 DO 对象 Person 和 User，其中 user 是 Person 的一个属性 ，一个 DTO 对象 PersonDTO 12345678910111213141516171819202122232425262728293031323334353637383940414243@NoArgsConstructor@AllArgsConstructor@Datapublic class Person &#123; private Long id; private String name; private String email; private Date birthday; private User user;&#125;@NoArgsConstructor@AllArgsConstructor@Datapublic class User &#123; private Integer age;&#125;@NoArgsConstructor@AllArgsConstructor@Datapublic class PersonDTO &#123; private Long id; private String name; /** * 对应 Person.user.age */ private Integer age; private String email; /** * 与 DO 里面的字段名称(birthDay)不一致 */ private Date birth; /** * 对 DO 里面的字段(birthDay)进行拓展,dateFormat 的形式 */ private String birthDateFormat; /** * 对 DO 里面的字段(birthDay)进行拓展,expression 的形式 */ private String birthExpressionFormat;&#125; 写一个 Mapper 接口 PersonConverter，其中两个方法，一个是单实体映射，另一个是List映射 若源对象属性与目标对象属性名字一致，会自动映射对应属性，不一样的需要指定，也可以用 format 转成自己想要的类型，也支持表达式的方式，可以看到像 id、name、email这些名词一致的我并没有指定 source-target，而birthday-birth指定了，转换格式的 birthDateFormat 加了dateFormat 或者 birthExpressionFormat 加了 expression，如果某个属性你不想映射，可以加个 ignore=true 1234567891011121314@Mapperpublic interface PersonConverter &#123; PersonConverter INSTANCE = Mappers.getMapper(PersonConverter.class); @Mappings(&#123; @Mapping(source = &quot;birthday&quot;, target = &quot;birth&quot;), @Mapping(source = &quot;birthday&quot;, target = &quot;birthDateFormat&quot;, dateFormat = &quot;yyyy-MM-dd HH:mm:ss&quot;), @Mapping(target = &quot;birthExpressionFormat&quot;, expression = &quot;java(org.apache.commons.lang3.time.DateFormatUtils.format(person.getBirthday(),\&quot;yyyy-MM-dd HH:mm:ss\&quot;))&quot;), @Mapping(source = &quot;user.age&quot;, target = &quot;age&quot;), @Mapping(target = &quot;email&quot;, ignore = true) &#125;) PersonDTO domain2dto(Person person); List&lt;PersonDTO&gt; domain2dto(List&lt;Person&gt; people);&#125; 编译MapStruct之后，手工编译或者启动 IDE 的时候 IDE 也会帮我们编译， 会自动在 target/classes 下生成对应的实现类 12手工编译命令mvn compile 注意！！！下面这个 PersonConverterImpl 是自动生成的，不是自己写的！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class PersonConverterImpl implements PersonConverter &#123; public PersonConverterImpl() &#123; &#125; public PersonDTO domain2dto(Person person) &#123; if (person == null) &#123; return null; &#125; else &#123; PersonDTO personDTO = new PersonDTO(); personDTO.setBirth(person.getBirthday()); if (person.getBirthday() != null) &#123; personDTO.setBirthDateFormat((new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;)).format(person.getBirthday())); &#125; Integer age = this.personUserAge(person); if (age != null) &#123; personDTO.setAge(age); &#125; personDTO.setId(person.getId()); personDTO.setName(person.getName()); personDTO.setBirthExpressionFormat(DateFormatUtils.format(person.getBirthday(), &quot;yyyy-MM-dd HH:mm:ss&quot;)); return personDTO; &#125; &#125; public List&lt;PersonDTO&gt; domain2dto(List&lt;Person&gt; people) &#123; if (people == null) &#123; return null; &#125; else &#123; List&lt;PersonDTO&gt; list = new ArrayList(people.size()); Iterator var3 = people.iterator(); while(var3.hasNext()) &#123; Person person = (Person)var3.next(); list.add(this.domain2dto(person)); &#125; return list; &#125; &#125; private Integer personUserAge(Person person) &#123; if (person == null) &#123; return null; &#125; else &#123; User user = person.getUser(); if (user == null) &#123; return null; &#125; else &#123; Integer age = user.getAge(); return age == null ? null : age; &#125; &#125; &#125;&#125; 写一个单元测试类 PersonConverterTest 测试一下，看看效果 12345678910111213141516171819public class PersonConverterTest &#123; @Test public void test() &#123; Person person = new Person(1L,&quot;zhige&quot;,&quot;zhige.me@gmail.com&quot;,new Date(),new User(1)); PersonDTO personDTO = PersonConverter.INSTANCE.domain2dto(person); assertNotNull(personDTO); assertEquals(personDTO.getId(), person.getId()); assertEquals(personDTO.getName(), person.getName()); assertEquals(personDTO.getBirth(), person.getBirthday()); String format = DateFormatUtils.format(personDTO.getBirth(), &quot;yyyy-MM-dd HH:mm:ss&quot;); assertEquals(personDTO.getBirthDateFormat(),format); assertEquals(personDTO.getBirthExpressionFormat(),format); List&lt;Person&gt; people = new ArrayList&lt;&gt;(); people.add(person); List&lt;PersonDTO&gt; personDTOs = PersonConverter.INSTANCE.domain2dto(people); assertNotNull(personDTOs); &#125;&#125; 多对一MapStruct 可以将几种类型的对象映射为另外一种类型，比如将多个 DO 对象转换为 DTO 例子 两个 DO 对象 Item 和 Sku，一个 DTO 对象 SkuDTO 123456789101112131415161718192021222324252627@NoArgsConstructor@AllArgsConstructor@Datapublic class Item &#123; private Long id; private String title;&#125;@NoArgsConstructor@AllArgsConstructor@Datapublic class Sku &#123; private Long id; private String code; private Integer price;&#125;@NoArgsConstructor@AllArgsConstructor@Datapublic class SkuDTO &#123; private Long skuId; private String skuCode; private Integer skuPrice; private Long itemId; private String itemName;&#125; 创建 ItemConverter（映射）接口，MapStruct 就会自动实现该接口 12345678910111213@Mapperpublic interface ItemConverter &#123; ItemConverter INSTANCE = Mappers.getMapper(ItemConverter.class); @Mappings(&#123; @Mapping(source = &quot;sku.id&quot;,target = &quot;skuId&quot;), @Mapping(source = &quot;sku.code&quot;,target = &quot;skuCode&quot;), @Mapping(source = &quot;sku.price&quot;,target = &quot;skuPrice&quot;), @Mapping(source = &quot;item.id&quot;,target = &quot;itemId&quot;), @Mapping(source = &quot;item.title&quot;,target = &quot;itemName&quot;) &#125;) SkuDTO domain2dto(Item item, Sku sku);&#125; 创建测试类，讲 Item 和 Sku 两个 DO对象，映射成一个 DTO 对象 SkuDTO 1234567891011121314public class ItemConverterTest &#123; @Test public void test() &#123; Item item = new Item(1L, &quot;iPhone X&quot;); Sku sku = new Sku(2L, &quot;phone12345&quot;, 1000000); SkuDTO skuDTO = ItemConverter.INSTANCE.domain2dto(item, sku); assertNotNull(skuDTO); assertEquals(skuDTO.getSkuId(),sku.getId()); assertEquals(skuDTO.getSkuCode(),sku.getCode()); assertEquals(skuDTO.getSkuPrice(),sku.getPrice()); assertEquals(skuDTO.getItemId(),item.getId()); assertEquals(skuDTO.getItemName(),item.getTitle()); &#125;&#125; 可以添加自定义方法1234567891011121314151617181920212223242526// 形式如下 default PersonDTO personToPersonDTO(Person person) &#123; //hand-written mapping logic&#125;// 比如在 PersonConverter 里面加入如下default Boolean convert2Bool(Integer value) &#123; if (value == null || value &lt; 1) &#123; return Boolean.FALSE; &#125; else &#123; return Boolean.TRUE; &#125;&#125;default Integer convert2Int(Boolean value) &#123; if (value == null) &#123; return null; &#125; if (Boolean.TRUE.equals(value)) &#123; return 1; &#125; return 0;&#125;// 测试类 PersonConverterTest 加入assertTrue(PersonConverter.INSTANCE.convert2Bool(1));assertEquals((int)PersonConverter.INSTANCE.convert2Int(true),1); 如果已经有了接收对象，更新目标对象1234567891011// 比如在 PersonConverter 里面加入如下，@InheritConfiguration 用于继承刚才的配置@InheritConfiguration(name = &quot;domain2dto&quot;)void update(Person person, @MappingTarget PersonDTO personDTO);// 测试类 PersonConverterTest 加入如下Person person = new Person(1L,&quot;zhige&quot;,&quot;zhige.me@gmail.com&quot;,new Date(),new User(1));PersonDTO personDTO = PersonConverter.INSTANCE.domain2dto(person);assertEquals(&quot;zhige&quot;, personDTO.getName());person.setName(&quot;xiaozhi&quot;);PersonConverter.INSTANCE.update(person, personDTO);assertEquals(&quot;xiaozhi&quot;, personDTO.getName()); Spring 注入的方式12// 刚才一直写的例子是默认的方式PersonConverter INSTANCE = Mappers.getMapper(PersonConverter.class); 还有一种常用的方式，是和常用的框架 Spring 结合，在 @Mapper 后面加入 componentModel=&quot;spring&quot; 1234567891011@Mapper(componentModel=&quot;spring&quot;)public interface PersonConverter &#123; @Mappings(&#123; @Mapping(source = &quot;birthday&quot;, target = &quot;birth&quot;), @Mapping(source = &quot;birthday&quot;, target = &quot;birthDateFormat&quot;, dateFormat = &quot;yyyy-MM-dd HH:mm:ss&quot;), @Mapping(target = &quot;birthExpressionFormat&quot;, expression = &quot;java(org.apache.commons.lang3.time.DateFormatUtils.format(person.getBirthday(),\&quot;yyyy-MM-dd HH:mm:ss\&quot;))&quot;), @Mapping(source = &quot;user.age&quot;, target = &quot;age&quot;), @Mapping(target = &quot;email&quot;, ignore = true) &#125;) PersonDTO domain2dto(Person person);&#125; 这时候测试类改一下，我用的 spring boot 的形式 123456789101112131415161718192021@RunWith(SpringRunner.class)@SpringBootTest(classes = BaseTestConfiguration.class)public class PersonConverterTest &#123; //这里把转换器装配进来 @Autowired private PersonConverter personConverter; @Test public void test() &#123; Person person = new Person(1L,&quot;zhige&quot;,&quot;zhige.me@gmail.com&quot;,new Date(),new User(1)); PersonDTO personDTO = personConverter.domain2dto(person); assertNotNull(personDTO); assertEquals(personDTO.getId(), person.getId()); assertEquals(personDTO.getName(), person.getName()); assertEquals(personDTO.getBirth(), person.getBirthday()); String format = DateFormatUtils.format(personDTO.getBirth(), &quot;yyyy-MM-dd HH:mm:ss&quot;); assertEquals(personDTO.getBirthDateFormat(),format); assertEquals(personDTO.getBirthExpressionFormat(),format); &#125;&#125; test 路径下加入了一个配置类 12345@EnableAutoConfiguration@Configuration@ComponentScanpublic class BaseTestConfiguration &#123;&#125; MapStruct 注解的关键词123456789101112@Mapper 只有在接口加上这个注解， MapStruct 才会去实现该接口 @Mapper 里有个 componentModel 属性，主要是指定实现类的类型，一般用到两个 default：默认，可以通过 Mappers.getMapper(Class) 方式获取实例对象 spring：在接口的实现类上自动添加注解 @Component，可通过 @Autowired 方式注入@Mapping：属性映射，若源对象属性与目标对象名字一致，会自动映射对应属性 source：源属性 target：目标属性 dateFormat：String 到 Date 日期之间相互转换，通过 SimpleDateFormat，该值为 SimpleDateFormat 的日期格式 ignore: 忽略这个字段@Mappings：配置多个@Mapping@MappingTarget 用于更新已有对象@InheritConfiguration 用于继承配置]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot配置日志]]></title>
    <url>%2F2019%2F06%2F19%2FSpringboot%E9%85%8D%E7%BD%AE%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[Spring boot 配置日志Spring Boot除了commons-logging API外没有其他强制性的日志依赖，你有很多可选的日志实现。想要使用Logback，你需要包含它，及一些对classpath下commons-logging的绑定。最简单的方式是通过依赖spring-boot-starter-logging的starter pom。对于一个web应用程序，你只需添加spring-boot-starter-web依赖，因为它依赖于logging starter。 例如，使用Maven： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; Spring Boot有一个LoggingSystem抽象，用于尝试通过classpath上下文配置日志系统。如果Logback可用，则首选它。如果你唯一需要做的就是设置不同日志的级别，那可以通过在application.properties中使用logging.level前缀实现，比如： 12logging.level.org.springframework.web: DEBUGlogging.level.org.hibernate: ERROR 你也可以使用logging.file设置日志文件的位置（除控制台之外，默认会输出到控制台）。 想要对日志系统进行更细粒度的配置，你需要使用正在说的LoggingSystem支持的原生配置格式。默认情况下，Spring Boot从系统的默认位置加载原生配置（比如对于Logback为classpath:logback.xml），但你可以使用logging.config属性设置配置文件的位置。 1. 配置Logback如果你将一个logback.xml放到classpath根目录下，那它将会被从这加载。Spring Boot提供一个默认的基本配置，如果你只是设置日志级别，那你可以包含它，比如： 12345&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;include resource="org/springframework/boot/logging/logback/base.xml"/&gt; &lt;logger name="org.springframework.web" level="DEBUG"/&gt;&lt;/configuration&gt; 如果查看spring-boot jar包中的默认logback.xml，你将会看到LoggingSystem为你创建的很多有用的系统属性，比如： ${PID}，当前进程id ${LOG_FILE}，如果在Boot外部配置中设置了logging.file ${LOG_PATH}，如果设置了logging.path（表示日志文件产生的目录） Spring Boot也提供使用自定义的Logback转换器在控制台上输出一些漂亮的彩色ANSI日志信息（不是日志文件）。具体参考默认的base.xml配置。 如果Groovy在classpath下，你也可以使用logback.groovy配置Logback。 2.配置Log4j2.1 常用配置方式Spring Boot也支持Log4j或Log4j 2作为日志配置，但只有在它们中的某个在classpath下存在的情况。如果你正在使用starter poms进行依赖装配，这意味着你需要排除Logback，然后包含你选择的Log4j版本。如果你不使用starter poms，那除了你选择的Log4j版本外还要提供commons-logging（至少）。 最简单的方式可能就是通过starter poms，尽管它需要排除一些依赖，比如，在Maven中： 123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt;&lt;/dependency&gt; 想要使用Log4j 2，只需要依赖spring-boot-starter-log4j2而不是spring-boot-starter-log4j。 注：使用Log4j各版本的starters都会收集好依赖以满足common logging的要求（比如，Tomcat中使用java.util.logging，但使用Log4j或 Log4j 2作为输出）。具体查看Actuator Log4j或Log4j 2的示例，了解如何将它用于实战。 2.2 使用Yaml或Json配置Log4j2除了它的默认XML配置格式，Log4j 2也支持YAML和JSON配置文件。想要使用其他配置文件格式来配置Log4j 2，你需要添加合适的依赖到classpath。为了使用YAML，你需要添加com.fasterxml.jackson.dataformat:jackson-dataformat-yaml依赖，Log4j 2将查找名称为log4j2.yaml或log4j2.yml的配置文件。为了使用JSON，你需要添加com.fasterxml.jackson.core:jackson-databind依赖，Log4j 2将查找名称为log4j2.json或log4j2.jsn的配置文件 转载自 Spring Boot参考指南]]></content>
      <categories>
        <category>spring全家桶</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[@Qualifier@Primary]]></title>
    <url>%2F2019%2F06%2F13%2FQualifier-Primary%2F</url>
    <content type="text"><![CDATA[@Qualifier @Primary的区别 当接口的实现类有多个的时候，同时使用@Component注解让Spring容器托管，使用类用@Autowired注解实现注入时候，Spring不知道注入哪一个，需要使用@Qualifier @Primary解决问题 区别@Primary：在众多相同的bean中，优先选择用@Primary注解的bean（该注解加在各个bean上） @Qualifier：在众多相同的bean中，@Qualifier指定需要注入的bean（该注解跟随在@Autowired后）]]></content>
      <categories>
        <category>spring全家桶</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker常用命令]]></title>
    <url>%2F2019%2F06%2F11%2FDocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Docker常用命令docker介绍Docker将应用程序与该程序的依赖，打包在一个文件里面。 image 文件生成的容器实例，本身也是一个文件，称为容器文件。 一旦容器生成，就会同时存在两个文件： image 文件和容器文件。 关闭容器并不会删除容器文件，只是容器停止运行而已，可以使用docker rm来删除。 docker操作命令列出所有image文件：docker images 下载镜像： docker image pull [image name] 查看镜像信息：docker inspect pengj/nodeappk8s:v1.0.0，查看镜像的某一部分信息：docker inspect -f .Size pengj/nodeappk8s:v1.0.0 查看镜像的具体内容：docker history pengj/nodeappk8s:v1.0.0 搜索镜像：docker search mysql，另外加上过滤参数docker search --filter=stars=100 mysql 删除镜像：docker rmi [image]，加上-f参数能够强制删除，即便有容器使用，一般不推荐 清理镜像：docker image prune 构建镜像： docker image build -t [username]/[repository]:[tag] 发布镜像：docker image push [username]/[repository]:[tag] 给镜像打标签, 使用tag命令添加镜像标签，指向同一镜像文件：docker tag ubuntu:lastest myubuntu:lastest 运行及进入容器：见👇【docker run和docker exec使用】 查看docker中所有的容器：docker ps -a 杀死某个容器：见👇【删除容器的方法汇总】 暂停某个容器：docker container stop [containID] 查看某个容器日志：见👇【查看日志docker logs使用】 查看日志docker logs使用docker logs -f -t --since=&quot;2017-05-31&quot; --tail=10 edu_web_1 --since: 此参数指定了输出日志开始日期，即只输出指定日期之后的日志。 -f: 查看实时日志 -t: 查看日志产生的日期 -tail=10: 查看最后的10条日志。 edu_web_1: 容器名称(容器名称) docker run和docker exec使用运行镜像，使用docker run docker run -it -p 8000:8000 -v /src/webapp(本地绝对):/opt/webapp(镜像绝对路径) ubuntu -i 打开标准输入接受用户的输入 -t 让docker分配一个伪终端(pseudo-tty)，并绑定到容器的标准输入上 -u 执行命令的用户 1234567docker run -p 容器端口:宿主端口 -v 宿主目录:容器目录 -d 后台运行 -e 设置环境变量 -name 容器名称 --rm 容器终止之后，自动删除文件复制代码 进入容器，查看状态，其中i,t参数说明和docker run一致 12docker exec -it [container_name/container_id] /bin/bash复制代码 删除容器的方法汇总 方法一： 12345#显示所有的容器，过滤出Exited状态的容器，取出这些容器的ID，sudo docker ps -a|grep Exited|awk &apos;&#123;print $1&#125;&apos;#查询所有的容器，过滤出Exited状态的容器，列出容器ID，删除这些容器sudo docker rm `docker ps -a|grep Exited|awk &apos;&#123;print $1&#125;&apos;`复制代码 方法二： 123#删除所有未运行的容器（已经运行的删除不了，未运行的就一起被删除了）sudo docker rm $(docker ps -a -q)复制代码 方法三： 123#根据容器的状态，删除Exited状态的容器sudo docker rm $(sudo docker ps -qf status=exited)复制代码 方法四： 123# 删除处于stop的容器或者状态status=Exited的容器sudo docker container prune复制代码 终止运行的容器文件，依然会占据硬盘空间，可以使用docker rm [containerID]命令删除。 docker 如何删除none镜像删除none的镜像，要先删除镜像中的容器。要删除镜像中的容器，必须先停止容器。 1234567# 停止容器docker stop $(docker ps -a | grep &quot;Exited&quot; | awk &apos;&#123;print $1 &#125;&apos;)# 删除容器docker rm $(docker ps -a | grep &quot;Exited&quot; | awk &apos;&#123;print $1 &#125;&apos;)# 删除镜像docker rmi $(docker images | grep &quot;none&quot; | awk &apos;&#123;print $3&#125;&apos;)复制代码 创建镜像的两种方法第一、基于已有的镜像创建主要是基于命令： 123456docker container commit -a, --author=&quot;&quot;: 作者信息； -c, --change=[]: 可以在提交的时候执行 Dockerfile 指令，如 CMD、ENTRYPOINT、ENV、EXPOSE、LABEL、ONBUILD、USER、VOLUME、WORIR 等； -m, --message=&quot;&quot;: 提交信息； -p, --pause=true: 提交时，暂停容器运行。复制代码 示例导入过程，需要记住当前容器的编号，假设为axa0c8cfec3a： 1234docker run -it xx:yy /bin/bashtouch news.txtexit复制代码 执行提交命令： docker container commit -m &quot;Added nes.txt file&quot; -a &quot;hapiman&quot; axa0c8cfec3a news:0.1 查看当前镜像列表：docker images 第二、基于 Dockerfile 来创建最常见的方式，略过 docker的导入导出操作 方法一、使用docker save和docker load 查看当前镜像列表：docker images 导出tar文件：docker save f59c7e5b1817 &gt;zwx_ub.tar 或者 docker save -o zwx_ub.tar f59c7e5b1817 加载镜像： docker load &lt; zwx_ub.tar 或者 docker load -i zwx_ub.tar 查看当前镜像列表：docker images 方法二、使用docker export和 docker import 导入：cat ubuntu-14.04-x86_64-minimal.tar.gz | docker import - ubuntu:zwx 导出：docker export 16f568766019 &gt; ubuntu.tar，将镜像编号为16f568766019的镜像导出 两者区别 docker save images_name：将一个镜像导出为文件，再使用docker load命令将文件导入为一个镜像，会保存该镜像的的所有历史记录。比docker export命令导出的文件大，很好理解，因为会保存镜像的所有历史记录。 docker export container_id：将一个容器导出为文件，再使用docker import命令将容器导入成为一个新的镜像，但是相比docker save命令，容器文件会丢失所有元数据和历史记录，仅保存容器当时的状态，相当于虚拟机快照]]></content>
      <categories>
        <category>部署运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql时间默认值]]></title>
    <url>%2F2019%2F06%2F11%2Fmysql%E6%97%B6%E9%97%B4%E9%BB%98%E8%AE%A4%E5%80%BC%2F</url>
    <content type="text"><![CDATA[MySQL 日期类型及默认设置 日期类型区别及用途MySQL 的日期类型有5个，分别是： date、time、year、datetime、timestamp。将在“菜鸟教程”和百度获取的资料，整理成如下表格： 类型 字节 格式 用途 是否支持设置系统默认值 date 3 YYYY-MM-DD 日期值 不支持 time 3 HH:MM:SS 时间值或持续时间 不支持 year 1 YYYY 年份 不支持 datetime 8 YYYY-MM-DD HH:MM:SS 日期和时间混合值 不支持 timestamp 4 YYYYMMDD HHMMSS 混合日期和时间，可作时间戳 支持 日期类型的 default 设置关于 default 设置，通常情况下会使用当前时间作为默认值。Example: 1ts_time timestamp NOT NULL DEFAULT NOW(); 1 or 1ts_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP(); 1 根据上表可知，除了 timestamp 类型支持系统默认值设置，其他类型都不支持。如果建表语句中有: 1234ts_time1 time NOT NULL DEFAULT NOW();ts_time3 yearNOT NULL DEFAULT NOW();ts_time2 date NOT NULL DEFAULT CURRENT_TIMESTAMP();ts_time2 datetime NOT NULL DEFAULT CURRENT_TIMESTAMP(); 1 2 3 4 都会报错。所以想要设置某个日期列的默认值为当前时间，只能使用 timestamp 类型，并设置 DEFAULT NOW() 或 DEFAULT CURRENT_TIMESTAMP() 作为默认值。 date 类型默认值使用 current_date() 创建失败 date 类型默认值使用 now() 创建失败 date 类型默认值使用 current_timestamp() 创建失败 datetime 类型默认值使用 current_timestamp() 创建失败 datetime 类型默认值使用 now() 创建失败 timestamp 类型默认值使用 now() 创建成功 常见的日期获取函数MySQL中有一些日期函数可供我们使用，我们可以使用 ” select 函数名() ; ” 的 sql 查看它们的返回值。同时也可以使用 “select 自定义函数名();”，调用我们自己定义的函数。Eample: 1select current_timestamp(); 1 1select current_date(); 1 1select current_time(); 1 1select now(); 1 参照“w3School相关内容”，其他常见的日期函数如下 函数 描述 NOW() 返回当前的日期和时间 CURDATE() 返回当前的日期 CURTIME() 返回当前的时间 DATE() 提取日期或日期/时间表达式的日期部分 EXTRACT() 返回日期/时间按的单独部分 DATE_ADD() 给日期添加指定的时间间隔 DATE_SUB() 从日期减去指定的时间间隔 DATEDIFF() 返回两个日期之间的天数 DATE_FORMAT() 用不同的格式显示日期/时间]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信公众号授权]]></title>
    <url>%2F2019%2F05%2F21%2F%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%8E%88%E6%9D%83%2F</url>
    <content type="text"><![CDATA[微信网页授权&lt;转载自微信公众号开发文档&gt; 如果用户在微信客户端中访问第三方网页，公众号可以通过微信网页授权机制，来获取用户基本信息，进而实现业务逻辑。 关于网页授权回调域名的说明 1、在微信公众号请求用户网页授权之前，开发者需要先到公众平台官网中的“开发 - 接口权限 - 网页服务 - 网页帐号 - 网页授权获取用户基本信息”的配置选项中，修改授权回调域名。请注意，这里填写的是域名（是一个字符串），而不是URL，因此请勿加 http:// 等协议头； 2、授权回调域名配置规范为全域名，比如需要网页授权的域名为：www.qq.com，配置以后此域名下面的页面http://www.qq.com/music.html 、 http://www.qq.com/login.html 都可以进行OAuth2.0鉴权。但http://pay.qq.com 、 http://music.qq.com 、 http://qq.com无法进行OAuth2.0鉴权 3、如果公众号登录授权给了第三方开发者来进行管理，则不必做任何设置，由第三方代替公众号实现网页授权即可 关于网页授权的两种scope的区别说明 1、以snsapi_base为scope发起的网页授权，是用来获取进入页面的用户的openid的，并且是静默授权并自动跳转到回调页的。用户感知的就是直接进入了回调页（往往是业务页面） 2、以snsapi_userinfo为scope发起的网页授权，是用来获取用户的基本信息的。但这种授权需要用户手动同意，并且由于用户同意过，所以无须关注，就可在授权后获取该用户的基本信息。 3、用户管理类接口中的“获取用户基本信息接口”，是在用户和公众号产生消息交互或关注后事件推送后，才能根据用户OpenID来获取用户基本信息。这个接口，包括其他微信接口，都是需要该用户（即openid）关注了公众号后，才能调用成功的。 关于网页授权access_token和普通access_token的区别 1、微信网页授权是通过OAuth2.0机制实现的，在用户授权给公众号后，公众号可以获取到一个网页授权特有的接口调用凭证（网页授权access_token），通过网页授权access_token可以进行授权后接口调用，如获取用户基本信息； 2、其他微信接口，需要通过基础支持中的“获取access_token”接口来获取到的普通access_token调用。 关于UnionID机制 1、请注意，网页授权获取用户基本信息也遵循UnionID机制。即如果开发者有在多个公众号，或在公众号、移动应用之间统一用户帐号的需求，需要前往微信开放平台（open.weixin.qq.com）绑定公众号后，才可利用UnionID机制来满足上述需求。 2、UnionID机制的作用说明：如果开发者拥有多个移动应用、网站应用和公众帐号，可通过获取用户基本信息中的unionid来区分用户的唯一性，因为同一用户，对同一个微信开放平台下的不同应用（移动应用、网站应用和公众帐号），unionid是相同的。 关于特殊场景下的静默授权 1、上面已经提到，对于以snsapi_base为scope的网页授权，就静默授权的，用户无感知； 2、对于已关注公众号的用户，如果用户从公众号的会话或者自定义菜单进入本公众号的网页授权页，即使是scope为snsapi_userinfo，也是静默授权，用户无感知。 具体而言，网页授权流程分为四步： 1、引导用户进入授权页面同意授权，获取code 2、通过code换取网页授权access_token（与基础支持中的access_token不同） 3、如果需要，开发者可以刷新网页授权access_token，避免过期 4、通过网页授权access_token和openid获取用户基本信息（支持UnionID机制） 目录 1 第一步：用户同意授权，获取code 2 第二步：通过code换取网页授权access_token 3 第三步：刷新access_token（如果需要） 4 第四步：拉取用户信息(需scope为 snsapi_userinfo) 5 附：检验授权凭证（access_token）是否有效 第一步：用户同意授权，获取code 在确保微信公众账号拥有授权作用域（scope参数）的权限的前提下（服务号获得高级接口后，默认拥有scope参数中的snsapi_base和snsapi_userinfo），引导关注者打开如下页面： 1https://open.weixin.qq.com/connect/oauth2/authorize?appid=APPID&amp;redirect_uri=REDIRECT_URI&amp;response_type=code&amp;scope=SCOPE&amp;state=STATE#wechat_redirect 若提示“该链接无法访问”，请检查参数是否填写错误，是否拥有scope参数对应的授权作用域权限。 尤其注意：由于授权操作安全等级较高，所以在发起授权请求时，微信会对授权链接做正则强匹配校验，如果链接的参数顺序不对，授权页面将无法正常访问 12345参考链接(请在微信客户端中打开此链接体验):scope为snsapi_basehttps://open.weixin.qq.com/connect/oauth2/authorize?appid=wx520c15f417810387&amp;redirect_uri=https%3A%2F%2Fchong.qq.com%2Fphp%2Findex.php%3Fd%3D%26c%3DwxAdapter%26m%3DmobileDeal%26showwxpaytitle%3D1%26vb2ctag%3D4_2030_5_1194_60&amp;response_type=code&amp;scope=snsapi_base&amp;state=123#wechat_redirectscope为snsapi_userinfohttps://open.weixin.qq.com/connect/oauth2/authorize?appid=wxf0e81c3bee622d60&amp;redirect_uri=http%3A%2F%2Fnba.bluewebgame.com%2Foauth_response.php&amp;response_type=code&amp;scope=snsapi_userinfo&amp;state=STATE#wechat_redirect 尤其注意：跳转回调redirect_uri，应当使用https链接来确保授权code的安全性。 参数说明 参数 是否必须 说明 appid 是 公众号的唯一标识 redirect_uri 是 授权后重定向的回调链接地址， 请使用 urlEncode 对链接进行处理 response_type 是 返回类型，请填写code scope 是 应用授权作用域，snsapi_base （不弹出授权页面，直接跳转，只能获取用户openid），snsapi_userinfo （弹出授权页面，可通过openid拿到昵称、性别、所在地。并且， 即使在未关注的情况下，只要用户授权，也能获取其信息 ） state 否 重定向后会带上state参数，开发者可以填写a-zA-Z0-9的参数值，最多128字节 #wechat_redirect 是 无论直接打开还是做页面302重定向时候，必须带此参数 下图为scope等于snsapi_userinfo时的授权页面： 用户同意授权后 如果用户同意授权，页面将跳转至 redirect_uri/?code=CODE&amp;state=STATE。 1code说明 ： code作为换取access_token的票据，每次用户授权带上的code将不一样，code只能使用一次，5分钟未被使用自动过期。 错误返回码说明如下： 返回码 说明 10003 redirect_uri域名与后台配置不一致 10004 此公众号被封禁 10005 此公众号并没有这些scope的权限 10006 必须关注此测试号 10009 操作太频繁了，请稍后重试 10010 scope不能为空 10011 redirect_uri不能为空 10012 appid不能为空 10013 state不能为空 10015 公众号未授权第三方平台，请检查授权状态 10016 不支持微信开放平台的Appid，请使用公众号Appid 第二步：通过code换取网页授权access_token 首先请注意，这里通过code换取的是一个特殊的网页授权access_token,与基础支持中的access_token（该access_token用于调用其他接口）不同。公众号可通过下述接口来获取网页授权access_token。如果网页授权的作用域为snsapi_base，则本步骤中获取到网页授权access_token的同时，也获取到了openid，snsapi_base式的网页授权流程即到此为止。 尤其注意：由于公众号的secret和获取到的access_token安全级别都非常高，必须只保存在服务器，不允许传给客户端。后续刷新access_token、通过access_token获取用户信息等步骤，也必须从服务器发起。 请求方法 1获取code后，请求以下链接获取access_token： https://api.weixin.qq.com/sns/oauth2/access_token?appid=APPID&amp;secret=SECRET&amp;code=CODE&amp;grant_type=authorization_code 参数说明 参数 是否必须 说明 appid 是 公众号的唯一标识 secret 是 公众号的appsecret code 是 填写第一步获取的code参数 grant_type 是 填写为authorization_code 返回说明 正确时返回的JSON数据包如下： 1234567&#123; &quot;access_token&quot;:&quot;ACCESS_TOKEN&quot;, &quot;expires_in&quot;:7200, &quot;refresh_token&quot;:&quot;REFRESH_TOKEN&quot;, &quot;openid&quot;:&quot;OPENID&quot;, &quot;scope&quot;:&quot;SCOPE&quot; &#125; 参数 描述 access_token 网页授权接口调用凭证,注意：此access_token与基础支持的access_token不同 expires_in access_token接口调用凭证超时时间，单位（秒） refresh_token 用户刷新access_token openid 用户唯一标识，请注意，在未关注公众号时，用户访问公众号的网页，也会产生一个用户和公众号唯一的OpenID scope 用户授权的作用域，使用逗号（,）分隔 错误时微信会返回JSON数据包如下（示例为Code无效错误）: 1&#123;&quot;errcode&quot;:40029,&quot;errmsg&quot;:&quot;invalid code&quot;&#125; 第三步：刷新access_token（如果需要） 由于access_token拥有较短的有效期，当access_token超时后，可以使用refresh_token进行刷新，refresh_token有效期为30天，当refresh_token失效之后，需要用户重新授权。 请求方法 12获取第二步的refresh_token后，请求以下链接获取access_token：https://api.weixin.qq.com/sns/oauth2/refresh_token?appid=APPID&amp;grant_type=refresh_token&amp;refresh_token=REFRESH_TOKEN 参数 是否必须 说明 appid 是 公众号的唯一标识 grant_type 是 填写为refresh_token refresh_token 是 填写通过access_token获取到的refresh_token参数 返回说明 正确时返回的JSON数据包如下： 1234567&#123; &quot;access_token&quot;:&quot;ACCESS_TOKEN&quot;, &quot;expires_in&quot;:7200, &quot;refresh_token&quot;:&quot;REFRESH_TOKEN&quot;, &quot;openid&quot;:&quot;OPENID&quot;, &quot;scope&quot;:&quot;SCOPE&quot; &#125; 参数 描述 access_token 网页授权接口调用凭证,注意：此access_token与基础支持的access_token不同 expires_in access_token接口调用凭证超时时间，单位（秒） refresh_token 用户刷新access_token openid 用户唯一标识 scope 用户授权的作用域，使用逗号（,）分隔 错误时微信会返回JSON数据包如下（示例为code无效错误）: 1&#123;&quot;errcode&quot;:40029,&quot;errmsg&quot;:&quot;invalid code&quot;&#125; 第四步：拉取用户信息(需scope为 snsapi_userinfo) 如果网页授权作用域为snsapi_userinfo，则此时开发者可以通过access_token和openid拉取用户信息了。 请求方法 1http：GET（请使用https协议） https://api.weixin.qq.com/sns/userinfo?access_token=ACCESS_TOKEN&amp;openid=OPENID&amp;lang=zh_CN 参数说明 参数 描述 access_token 网页授权接口调用凭证,注意：此access_token与基础支持的access_token不同 openid 用户的唯一标识 lang 返回国家地区语言版本，zh_CN 简体，zh_TW 繁体，en 英语 返回说明 正确时返回的JSON数据包如下： 1234567891011&#123; &quot;openid&quot;:&quot; OPENID&quot;, &quot; nickname&quot;: NICKNAME, &quot;sex&quot;:&quot;1&quot;, &quot;province&quot;:&quot;PROVINCE&quot; &quot;city&quot;:&quot;CITY&quot;, &quot;country&quot;:&quot;COUNTRY&quot;, &quot;headimgurl&quot;: &quot;http://thirdwx.qlogo.cn/mmopen/g3MonUZtNHkdmzicIlibx6iaFqAc56vxLSUfpb6n5WKSYVY0ChQKkiaJSgQ1dZuTOgvLLrhJbERQQ4eMsv84eavHiaiceqxibJxCfHe/46&quot;, &quot;privilege&quot;:[ &quot;PRIVILEGE1&quot; &quot;PRIVILEGE2&quot; ], &quot;unionid&quot;: &quot;o6_bmasdasdsad6_2sgVt7hMZOPfL&quot;&#125; 参数 描述 openid 用户的唯一标识 nickname 用户昵称 sex 用户的性别，值为1时是男性，值为2时是女性，值为0时是未知 province 用户个人资料填写的省份 city 普通用户个人资料填写的城市 country 国家，如中国为CN headimgurl 用户头像，最后一个数值代表正方形头像大小（有0、46、64、96、132数值可选，0代表640*640正方形头像），用户没有头像时该项为空。若用户更换头像，原有头像URL将失效。 privilege 用户特权信息，json 数组，如微信沃卡用户为（chinaunicom） unionid 只有在用户将公众号绑定到微信开放平台帐号后，才会出现该字段。 错误时微信会返回JSON数据包如下（示例为openid无效）: 1&#123;&quot;errcode&quot;:40003,&quot;errmsg&quot;:&quot; invalid openid &quot;&#125; 附：检验授权凭证（access_token）是否有效 请求方法 1http：GET（请使用https协议） https://api.weixin.qq.com/sns/auth?access_token=ACCESS_TOKEN&amp;openid=OPENID 参数说明 参数 描述 access_token 网页授权接口调用凭证,注意：此access_token与基础支持的access_token不同 openid 用户的唯一标识 返回说明正确的JSON返回结果： 1&#123; &quot;errcode&quot;:0,&quot;errmsg&quot;:&quot;ok&quot;&#125; 错误时的JSON返回示例： 1&#123; "errcode":40003,"errmsg":"invalid openid"&#125;]]></content>
      <categories>
        <category>技术杂记</category>
      </categories>
      <tags>
        <tag>微信平台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概要设计模板]]></title>
    <url>%2F2019%2F05%2F14%2F%E6%A6%82%E8%A6%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E6%9D%BF%2F</url>
    <content type="text"><![CDATA[1.引言1.1 编写目的【设计软件结构的具体任务是将一个复杂系统按功能进行模块划分、建立模块的层次结构及调用关系、确定模块间的接口及人机界面等。数据结构设计包括数据特征的描述、确定数据的结构特性、以及数据库的设计。此概要设计说明书是为了说明整个系统的体系架构，以及需求用例的各个功能点在架构中的体现，为系统的详细设计人员进行详细设计师的输入参考文档。】 1.2 背景1.3 定义【列出文本中用到的专门术语定义和外文首字母的原词组】 2.总体设计2.1 需求规定2.2 运行环境【软件系统运行环境】 2.3 基本设计概念和处理流程2.4 结构【说明本系统的系统元素（各层模块，子程序，公用程序等）划分，扼要说明每个系统元素的标识符和功能，分层次地给出各个元素之间的控制和被控制关系】 2.4 功能起与程序的关系2.3 人工处理流程【说明在本软甲你系统的工作过程中不得不包含的人工处理过程】 2.4 尚未解决问题3.接口设计3.1 用户接口3.2 外部接口3.3 内部接口4.接口设计4.1 运行模块组合4.2 运行控制4.3 运行时间5.系统数据结构设计5.1 逻辑结构设计要点5.2 物理结构设计要点5.3 数据结构与程序的关系6.系统出错处理设计6.1 出错信息6.2 补救措施【说明故障出现后可能采取的变通措施】 6.3 系统维护设计]]></content>
      <categories>
        <category>设计与架构</category>
      </categories>
      <tags>
        <tag>设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql深入浅出-索引（-）]]></title>
    <url>%2F2019%2F05%2F10%2FMysql%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-%E7%B4%A2%E5%BC%95%EF%BC%88-%EF%BC%89%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kotlin范围函数比较]]></title>
    <url>%2F2019%2F04%2F29%2FKotlin%E8%8C%83%E5%9B%B4%E5%87%BD%E6%95%B0%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>kotlin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐书单]]></title>
    <url>%2F2019%2F04%2F22%2F%E6%8E%A8%E8%8D%90%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[成长篇《异类》 一句话推荐：颠覆你对成功的认知，例如：什么才是赢在起跑线？为何现在的富人都是大约生于 1955 年左右？ 《随机漫步的傻瓜》 一句话推荐：只要看这一本书，你就能免受所有鸡汤的毒害！ 《一万小时天才理论》 一句话推荐：1 万小时理论实践版，详细阐述了 1 万小时天才理论的 3 个关键点。 《情商》 一句话推荐：如果你认为你的老板还不如你聪明，那你需要好好看看这本书。 《优秀到不能被忽视》 一句话推荐：不管是工作还是爱好，要想成功的原则是什么？很简单，“做别人愿意买单的事情”！ 《影响力大师》 一句话推荐：天天立 flag，月月打自己的脸？不是你意志力不行，而是你方法不对，这本书可以给你一套完善、可操作的方法。（注：我以前读的版本叫《关键影响力》，新版改名叫《影响力大师》。） 技术篇推荐技术书籍实际上是有一定局限性的，因为每个技术领域其实差异还是挺大的，就算都叫程序员，前端程序员、客户端程序员、后端程序员之间差异就很大；即使都是后端程序员，Linux 开发和 Windows 开发所需要的技术也不一样。因此我提炼了一个通用的技术书籍学习路径，不同技术领域可以按照这个路径去拆解： 深度学习你的代码运行环境：例如 Linux 程序员一定要深入学习 Linux 和 UNIX 的操作系统，iOS 程序员要深入学习 iOS 系统，前端程序员要深入学习浏览器原理，以此类推。 深入学习你的核心工具：例如 Java 程序员的核心工具是 Java，嵌入式程序员是 C，而 DBA 就不是学编程语言，而是学 MySQL 或者 Oracle 了。 深度学习领域基础知识：例如后端程序员的网络编程，前端程序员的动效知识，Android 客户端程序员的渲染知识，以及所有程序员都要求的算法知识等。 广泛学习技术领域的通用成熟技术：例如前端程序员要学的 React 和 Vue，Java 程序员要学的 Netty、Spring，互联网后端程序员的标配 MySQL、Redis 等。 下面我以 Linux 后端 Java 程序员为例，给你推荐相关技术书籍。 《UNIX 编程艺术》 一句话推荐：经典书籍，结合 UNIX 的历史来讲 UNIX 设计哲学，改变你对编程的认知和理解。 《UNIX 网络编程（卷 1）》 一句话推荐：经典书籍，网络编程必读。书很厚，重点是前三部分，不需要一次全部读懂，先通读，后面经常参考并且加深理解。 《UNIX 环境高级编程》 一句话推荐：经典书籍，Linux/UNIX C/C++ 程序员必读，就算是 Java、PHP、Python 等程序员也要通读一遍，了解系统底层能力有助于理解编程语言的各种实现。 《Linux 系统编程》 一句话推荐：和《UNIX 环境高级编程》类似，Linux 平台可以看这本。 《TCP/IP 详解（卷 1）》 一句话推荐：经典书籍，全面介绍 TCP/IP 协议栈各种协议，重点看 TCP 和 IP 部分。 《算法之美》 一句话推荐：讲算法非常有趣的一本书，告诉你如何将算法应用于恋爱、生活、工作！ 《算法设计与应用》 一句话推荐：将算法与实际应用结合起来，从应用引出算法然后进行算法推理，如果你数学很牛，可以挑战一下这本书；如果你数学很菜，那我更加推荐这本书，因为其中的算法原理和应用场景分析得清晰易懂。 《Java 编程思想》 一句话推荐：经典书籍，全面介绍 Java 编程，入门必备。 《深入理解 Java 虚拟机》 一句话推荐：全面理解 Java 虚拟机，原理介绍得深入浅出，很少有技术书籍我会优先推荐国内作者，而这本是我大力推荐的。 《C++ Primer》 一句话推荐：经典书籍，全面介绍 C++ 编程。当年我看了很多 C++ 书籍都不得要领，看了这本后豁然开朗。 业务篇不管是普通程序员还是架构师，实践工作中都需要有一定的业务理解能力，而架构师的业务理解能力要求更高。理解业务一方面有利于更好地设计有针对性的架构或者方案，另外一方面也可以防止被产品经理坑 ：） 《增长黑客》 一句话推荐：肖恩·埃利斯和摩根·布朗的这本书理论体系完整，既给出了很多实践技巧，又总结了很多经验和需要避开的陷阱。 《需求》 一句话推荐：如何理解用户需求、如何满足用户需求、同样产品为何有的公司失败而有的公司取得了巨大成功？这本书让我茅塞顿开，建议技术同学都推荐这本书给你们的产品经理。 《淘宝十年产品事》 一句话推荐：这本书总结了淘宝 10 多年发展过程中产品遇到的各种坑和挑战，让你明白“罗马不是一天建成的”，产品也是逐步演化的（这也是我的“架构设计三原则”中的“演化原则”）。 《定位》 一句话推荐：告诉你如何做业务战略规划，有些偏重理论，架构师需要学习，程序员可以先放一边。 《宝洁制胜战略》 一句话推荐：结合宝洁的经验，提出了一套完善的战略规划和落地方法，理论与实践兼备，架构师必备，拿着这套方法论，就可以 PK 你的老板了。]]></content>
      <categories>
        <category>技术杂记</category>
      </categories>
      <tags>
        <tag>书单</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[架构是什么]]></title>
    <url>%2F2019%2F04%2F15%2F%E6%9E%B6%E6%9E%84%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[架构是什么？ 软件架构是有关软件整体结构与组件的抽象描述，用于指导大型软件系统各个方面的设计。 –《维基百科》 要理清架构的概念，关键要梳理几个有关系而又相似的概念，包括：系统与子系统，模块与组件，框架与架构 1.系统与子系统的关系 系统泛指由一群有关联的个体组成，根据某种规则运作，能完成个别元件不能单独完成的工作的群体。它的意思是“总体”“整体”或“联盟” 子系统也是由一群有关联的个体所组成的系统，多半会是更大系统中的一部分 –《维基百科》 子系统的定义和系统定义是一样的，只是观察的角度有差异，一个系统可能是另外一个更大系统的子系统。 例如微信系统的层级结构 2.模块与组件的关系 软件模块（Module）是一套一致而互相有紧密关连的软件组织。它分别包含了程序和数据结构两部分。现代软件开发往往利用模块作为合成的单位。模块的接口表达了由该模块提供的功能和调用它时所需的元素。模块是可能分开被编写的单位。这使它们可再用和允许人员同时协作、编写及研究不同的模块。 软件组件定义为自包含的、可编程的、可重用的、与语言无关的软件单元，软件组件可以很容易被用于组装应用程序中 –《维基百科》 模块和组件都是系统的组成部分，只是从不同的角度拆分系统而已。 模块是从逻辑角度去看待，而组件是从物理角度去看待 划分模块的主要目的是职责分离，划分组件的主要目的是单元复用 3.架构与框架的关系 软件框架（Software framework）通常指的是为了实现某个业界标准或完成特定基本任务的软件组件规范，也指为了实现某个软件组件规范时，提供规范所要求之基础功能的软件产品。 软件架构指软件系统的“基础结构”，创造这些基础结构的准则，以及对这些结构的描述。 –《维基百科》 框架是规范也是约束，可以理解为封闭性的话题，定义好，让别人如何去使用，而架构是一种结构，是一种开放性的话题，如何去设计组织架构，如何让架构更具有拓展性，减少沟通错误成本 框架关注的是“规范”，架构关注的是“结构” 框架的英文是Framework，架构的英文是Architecture 4.架构采用不同的角度或者维度，可以将系统划分为不同的结构这些“架构”，以下都是“学生管理系统”正确的架构，只是从不同的角度来分解而已 从业务逻辑的角度分解，“学生管理系统”的架构是： 从物理部署的角度分解，“学生管理系统”的架构是： 从开发规范的角度分解，“学生管理系统”可以采用标准的 MVC 框架来开发，因此架构又变成了 MVC 架构： 总结：软件架构指软件系统的顶层结构 “系统是一群关联个体组成”，这些“个体”可以是“子系统”“模块”“组件”等；架构需要明确系统包含哪些“个体” 系统中的个体需要“根据某种规则”运作，架构需要明确个体运作和协作的规则]]></content>
      <categories>
        <category>设计与架构</category>
      </categories>
      <tags>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最基础的数据结构-数组]]></title>
    <url>%2F2019%2F04%2F14%2F%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[最基础的数据结构–数组每一种编程语言，基本上都有数组这种数据类型。它不单单是一种数据类型，同时也是一个数据结构。 数组用一块连续的内存空间，来存储相同类型的一组数据,最大的特点就是支持随机访问，但插入、删除操作效率很低，平均时间复杂度未O(n) 1.数组如何实现随机访问1.1 数组的特性数组是一种线性表数据结构。用一组连续的内存空间，来存储相同类型的数据。 支持随机访问能力因为其有两个特性： 线性表，即数据排成像一条线一样的机构 连续的内存空间和相同类型的数据 1.2实现随机访问方式我们拿一个长度为 10 的 int 类型的数组 int[] a = new int[10] 来举例。在我画的这个图中，计算机给数组 a[10]，分配了一块连续内存空间 1000～1039，其中，内存块的首地址为 base_address = 1000。 我们知道，计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址： 12a[i]_address = base_address + i * data_type_size复制代码 其中 data_type_size 表示数组中每个元素的大小。我们举的这个例子里，数组中存储的是 int 类型数据，所以 data_type_size 就为 4 个字节。 2.低效的插入和删除2.1 插入和删除涉及的操作数组的插入和删除操作都可能会涉及其它数组元素的移动，这是造成低效的根本原因。 假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位。 如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 (1+2+…n)/n=O(n)。 2.2插入和删除特定情况下的优化2.2.1 插入操作优化如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移 k 之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合。在这种情况下，如果要将某个数组插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置。 为了更好地理解，我们举一个例子。假设数组 a[10] 中存储了如下 5 个元素：a，b，c，d，e。 我们现在需要将元素 x 插入到第 3 个位置。我们只需要将 c 放入到 a[5]，将 a[2] 赋值为 x 即可。最后，数组中的元素如下： a，b，x，d，e，c。 利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。这个处理思想在快排中也会用到 2.2.2 删除操作优化跟插入数据类似，如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。 和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；如果删除开头的数据，则最坏情况时间复杂度为 O(n)；平均情况时间复杂度也为 O(n)。 实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？ 我们继续来看例子。数组 a[10] 中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。 为了避免 d，e，f，g，h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。 这样的操作类似于JVM标记清楚垃圾回收算法的核心思想 3.使用数组最常见的问题：数组越界数组越界为访问数组时候越出了当初分配给数组的地址空间，C语言数组越界比较特殊，运行时不会提醒。其他语言，例如Java会抛出java.lang.ArrayIndexOutOfBoundsException异常。很多计算机病毒也正是利用到了代码中的数组越界可以访问非法地址的漏洞，来攻击系统，所以写代码的时候一定要警惕数组越界 4.容器&amp;&amp;数组ArrayList 最大的优势就是可以将很多数组操作的细节封装起来。比如前面提到的数组插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容，因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。 数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。如果需要增加数组容量，只能重新分配一块更大的空间，将原来的数据复制过去，然后再插入新的数据。 使用数组和容器场景： Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。 如果要表示多维数组，用数组会更加直观。 对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。 5.数组索引要从0开始，而不是15.1特殊的内存模型下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用这个公式： 12a[k]_address = base_address + k * type_size复制代码 但是，如果数组从 1 开始计数，那我们计算数组元素 a[k] 的内存地址就会变为： 12a[k]_address = base_address + (k-1)*type_size复制代码 对比两个公式，我们不难发现，从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。 数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。 5.2历史原因C语言一开始即如此，其它语言效仿之，当然也有其他语言并不是，例如Matlab，Python]]></content>
      <categories>
        <category>算法与模式</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性表和非线性表]]></title>
    <url>%2F2019%2F04%2F14%2F%E7%BA%BF%E6%80%A7%E8%A1%A8%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[线性表和非线性表 线性表，Linear List，即数据排成像线一样的结构，每个线性表上的数据最多只有前和后的两个方向 非线性表，与线性表对立，在非线性表中，数据并不是简单的前后关系]]></content>
      <categories>
        <category>算法与模式</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂度分析（空间复杂度）（二）]]></title>
    <url>%2F2019%2F04%2F13%2F%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90%EF%BC%88%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%EF%BC%89%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[（二）复杂度分析（空间复杂度）空间复杂度全称是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。 12345678void fuction(n)&#123; int i = 0; int[] a = new int[n]; for(i;i&lt;n;i++)&#123; a[i] = i*i; &#125; //use a[i] to do someting&#125; 例如上面的代码，第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。第 3 行申请了一个大小为 n 的 int 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 我们常见的空间复杂度就是 O(1)、O(n)、O(n2 )，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。 总结复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(logn)、O(n)、O(nlogn)、O(n2 )。等你学完整个专栏之后，你就会发现几乎所有的数据结构和算法的复杂度都跑不出这几个。 其它重要概念 最好情况时间复杂度（best case time complexity） 最坏情况时间复杂度（worst case time complexity） 平均情况时间复杂度（average case time complexity） 均摊时间复杂度（amortized time complexity）]]></content>
      <categories>
        <category>算法与模式</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复杂度分析（时间复杂度）（一）]]></title>
    <url>%2F2019%2F04%2F13%2F%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90%EF%BC%88%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%EF%BC%89%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[复杂度–时间复杂度 数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行得更快，如何让代码更加节省空间。 1.复杂度分析的必要性如果不采用事前的复杂度预估分析，只有采取事后的统计法。事后统计即把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。 但事后统计有非常大的局限性： 测试结果非常依赖环境（没有统一的标准） 测试结果受数据规模，结构的影响很大（例如不同排序算法） 2.大O复杂度表示法 T(n)表示代码执行的时间，n表示数据规模大小,f(n)表示每行代码执行次数的总和。从上图公式可看出，代码的执行时间T(n)与f(n)表达式成正比。 大O时间复杂度实际上并不表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。 而当n很大时候，公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可忽略，我们只需要记录一个最大量级就可以了。例如T(n) = O(n)； T(n) = O(n²) 3.时间复杂度分析方法 只关注循环执行次数最多的一段代码 加法法则：总复杂度等于量级最大的那段代码的复杂度 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积 4.几种常见时间复杂度实例分析 复杂度量级可粗略的分为：多项式量级和非多项式量级。 我们把时间复杂度为非多项式量级的算法问题叫作NP（Non-Deterministic Polynomial，非确定多项式）问题，非多项式量级只有两个： O(2n) O(n!) 总的来说：当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。 5.常见的多项式时间复杂度1. O(1) 首先你必须明确一个概念，O(1) 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。比如这段代码，即便有 3 行，它的时间复杂度也是 O(1），而不是 O(3)。 123int i = 8;int j = 6;int sum = i + j; 我稍微总结一下，只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。 2. O(logn)、O(nlogn) 对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。我通过一个例子来说明一下。 1234i=1;while (i &lt;= n) &#123; i = i * 2;&#125; 根据我们前面讲的复杂度分析方法，第三行代码是循环执行次数最多的。所以，我们只要能计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。 从代码中可以看出，变量 i 的值从 1 开始取，每循环一次就乘以 2。当大于 n 时，循环结束。还记得我们高中学过的等比数列吗？实际上，变量 i 的取值就是一个等比数列。如果我把它一个一个列出来，就应该是这个样子的： 所以，我们只要知道 x 值是多少，就知道这行代码执行的次数了。通过 2x=n 求解 x 这个问题我们想高中应该就学过了，我就不多说了。x=log2n，所以，这段代码的时间复杂度就是 O(log2n)。 现在，我把代码稍微改下，你再看看，这段代码的时间复杂度是多少？ 1234i=1;while (i &lt;= n) &#123; i = i * 3;&#125; 根据我刚刚讲的思路，很简单就能看出来，这段代码的时间复杂度为 O(log3n)。 实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。为什么呢？ 我们知道，对数之间是可以互相转换的，log3n 就等于 log32 log2n，所以 O(log3n) = O(C log2n)，其中 C=log32 是一个常量。基于我们前面的一个理论：在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))。所以，O(log2n) 就等于 O(log3n)。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 O(logn)。 如果你理解了我前面讲的 O(logn)，那 O(nlogn) 就很容易理解了。还记得我们刚讲的乘法法则吗？如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了。而且，O(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是 O(nlogn)。 3. O(m+n)、O(m*n) 我们再来讲一种跟前面都不一样的时间复杂度，代码的复杂度由两个数据的规模来决定。老规矩，先看代码！ 123456789101112131415int cal(int m, int n) &#123; int sum_1 = 0; int i = 1; for (; i &lt; m; ++i) &#123; sum_1 = sum_1 + i; &#125; int sum_2 = 0; int j = 1; for (; j &lt; n; ++j) &#123; sum_2 = sum_2 + j; &#125; return sum_1 + sum_2;&#125; 从代码中可以看出，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 O(m+n)。 针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)T2(n) = O(f(m) f(n))。]]></content>
      <categories>
        <category>算法与模式</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filter Diff Interceptor]]></title>
    <url>%2F2019%2F03%2F15%2FFilter-Diff-Interceptor%2F</url>
    <content type="text"><![CDATA[Filter Diff InterceptorReference StackOverflow: HandlerInterceptor is basically similar to a Servlet Filter, but in contrast to the latter it just allows custom pre-processing with the option of prohibiting the execution of the handler itself, and custom post-processing. Filters are more powerful, for example they allow for exchanging the request and response objects that are handed down the chain. Note that a filter gets configured in web.xml, a HandlerInterceptor in the application context. As a basic guideline, fine-grained handler-related preprocessing tasks are candidates for HandlerInterceptor implementations, especially factored-out common handler code and authorization checks. On the other hand, a Filter is well-suited for request content and view content handling, like multipart forms and GZIP compression. This typically shows when one needs to map the filter to certain content types (e.g. images), or to all requests. 1. Filter Filter是servlet规范中定义的java web组件, 在所有支持java web的容器中都可以使用 Filter和Filter Chain是密不可分的, Filter可以实现依次调用正是因为有了Filter Chain Filter 调用链 上图是Filter对请求进行拦截的原理图 2.Interceptor Interceptor不是servlet规范中的java web组件, 而是Spring提供的组件 Interceptor功能的实现主要是在Spring Mvc的DispatcherServelt.doDispatch方法中 拦截器流程 调用拦截器的前置方法 -&gt; 调用处理请求的方法 -&gt; 渲染模版 -&gt; 调用拦截器的后置处理方法 -&gt; 调用拦截器的完成方法 3.Summary 从以上分析可以看到过滤器和拦截器实现的方式的不同. Filter是利用了方法的调用(入栈出栈)完成整个流程, 而Interceptor是利用了for循环完成了整个流程. Filter的实现比较占用栈空间, 在Filter多的情况下可能会有栈溢出的风险存在. Interceptor的实现逻辑更加的清晰简单 Filter组件更加的通用, 只要支持java servlet的容器都可以使用, 而Interceptor必须依赖于Spring Filter的优先级是高于Interceptor, 即请求是先到Filter再到Interceptor的, 因为Interceptor的实现主体还是一个servlet]]></content>
      <categories>
        <category>spring全家桶</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是SSL？]]></title>
    <url>%2F2019%2F03%2F13%2F%E4%BB%80%E4%B9%88%E6%98%AFSSL%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[SSL （安全套接字层） SSL是用在Web服务器和浏览器之间建立加密链接的标准安全技术，可确保在Web服务器和浏览器之间传递的所有数据保持私有和完整。SSL是行业标准，数千万的网站使用SSL来保护与客户的在线交易。 1.申请SSL证书 为了能够创建SSL连接，Web服务器需要SSL证书。当您选择在Web服务器上激活SSL时，系统将提示您完成有关您的网站和公司身份的许多问题。然后，您的Web服务器将创建两个加密密钥 - 私钥和公钥 公钥不需要保密，并放入证书签名请求（CSR） - 一个包含您的详细信息的数据文件。然后，您应该提交CSR。在SSL证书申请过程中，证书颁发机构将验证您的详细信息并颁发包含您的详细信息的SSL证书，并允许您使用SSL ​ 公钥证书 2.使用SSL证书 浏览器发送SSL证书与web服务器的私钥相匹配，然后，您的Web服务器将能够在网站和客户的Web浏览器之间建立加密链接 SSL协议的复杂性对您的客户来说仍然是不可见的。相反，他们的浏览器为他们提供了一个关键指示器，让他们知道他们当前受到SSL加密会话的保护 - 右下角的锁定图标，点击锁定图标会显示您的SSL证书及其详细信息。所有SSL证书均颁发给公司或法律责任人 3.总结 通常，SSL证书将包含您的域名，公司名称，地址，城市，州和国家/地区。它还将包含证书的到期日期以及负责颁发证书的证书颁发机构的详细信息 当浏览器连接到安全站点时，它将检索站点的SSL证书并检查它是否已过期，它是由浏览器信任的证书颁发机构颁发的，并且它已由发布它的网站使用 如果在这些检查中的任何一个检查失败，浏览器将向最终用户显示警告，告知他们该网站不受SSL保护]]></content>
      <categories>
        <category>技术杂记</category>
      </categories>
      <tags>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[处方外流]]></title>
    <url>%2F2019%2F03%2F12%2F%E5%A4%84%E6%96%B9%E5%A4%96%E6%B5%81%2F</url>
    <content type="text"><![CDATA[处方外流 1. DTP（Direct to Patient）药品直接到达患者 即高值药品直送，由药企直接将高值药品配送到相关零售门店进行销售，省去代理商，商业利润更高。患者在拿到医院处方后可以在药房买到药物并获得专业的用药指导 DTP模式更依赖与药企的资源对接，以及更高的药事服务能力。其产品主要以高毛利的专业药、新特药为主，且多属于自费药品 DTP药房突出企业：上海医药（收购康德乐）、国大药房、老百姓大药房等2.药房托管（类似于学校的食堂外包） 医疗机构把药房经营权外包给有资质的企业经营，收取经营企业有偿经营和管理费用。是一种明晰药房所有者与经营者之间权力义务关系的以中药品经营模式特点是以医院为主导，医药流通企业承办居多，属于过度模式 由于未能完全破除医院和托管药房之间的利益关系，且存在托管费用过高等问题，目前托管药房模式正遭遇尴尬境地，行业非议之声不绝。3.院边店 由于其处于医院附近，地理位置优势将享受处方外流的红利4.互联网+医药新零售（医药O2O模式）（卖的更多，买的更好） 特点是满足患者购药的便捷性，药品品类比较单一，集中于非处方药和健康产品，处方来源是最重要的限制因素 目的：利用新技术充分挖掘用户的消费潜力、或为用户提供更细致周到的商品和服务 实现方式：大数据驱动、智能化、线上线下结合、供应链改造、用户体验升级 企业：阿里健康O2O联盟、京东到家、好药师、叮当快药、快方送药、生鲜外卖平台5.药店+诊所模式（类似于中医坐堂+药店模式） 药店内设诊所，提供基础医疗服务和慢病续方在互联网医疗发展起来之后，形成了远程诊疗+药店的模式，包括微医药诊店、微问诊等，其充分利用了医疗资源，为居民提供了轻问诊和电子处方6.院外处方流转平台 第三方公司搭建平台，连接医疗结构和药房，提供信息化、患者管理、数据管理等服务 总结 零售药店尤其是连锁零售药店是处方外流的最大获益者零售药店拥有非常好的业务基础，能够有序承接患者对药品和药事服务的需求；国内零售药店覆盖率较高，可顺势成为居民购药的首选； 在信息化工具、处方流转平台等助力下，零售药店的竞争力正在得到加强，能够为居民提供更多样化、精准的医药和健康管理服务问题 门槛：处方来源、患者意愿、医保对接、服务能力、信息管理]]></content>
      <categories>
        <category>行业领域</category>
      </categories>
      <tags>
        <tag>医药</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关系数据模型&对象模型驱动]]></title>
    <url>%2F2019%2F03%2F11%2F%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B-%E5%AF%B9%E8%B1%A1%E6%A8%A1%E5%9E%8B%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[数据关系模型和对象模型 数据驱动、模型驱动作为如今软件设计中两种不同的模型驱动方法，应该说各有各的优缺点以及适用的场合，不能就一概的去认为哪种必然就是更好的。 1.数据关系模型驱动 数据驱动采用的方式是根据对业务的分析建立数据模型来进行系统设计的一种方法，通过数据模型的建立来完成系统的实现，一般来说，在采取数据模型的系统中多采用的是前台直接和数据模型进行绑定的方式，这样在实现起来相对来讲会非常的快速。根据数据驱动的系统设计以及实现方式上来讲，数据驱动适合于数据型的应用系统的建设，而现在大部分的中小型应用系统很多就停留在这个层面上，在这类系统中数据驱动会显得特别的实用和好用，这类系统一个非常突出的共同点就是系统基本属于信息的录入、显示以及查询这样的一个过程，不存在复杂的数据业务逻辑处理。 2.对象模型驱动 ​ 模型驱动采用的方式根据对业务的分析建立业务对象模型来进行系统设计的一种方法，通过业务对象模型结合系统架构约束来进行系统的实现，一般来说，在采取模型驱动的系统中多采用N层的结构体系，前台显示一般和业务显示模型进行交互，而业务显示模型则通过业务对象模型进行交互来完成业务逻辑的处理，业务对象模型通过与持久对象模型进行业务持久的处理，在这样的情况下，势必增加了系统的复杂度，模型驱动适合与业务型应用系统的建设，这个在行业化的业务应用上显得比较突出，这类系统的共同点在于业务逻辑较为复杂而且多变，系统不仅仅是信息的录入、显示以及查询，更多的是对录入或显示的信息进行业务逻辑的处理。经过上面的简单介绍后，我觉得对于数据驱动和模型驱动都会有个大概的概念，只能说数据驱动和模型驱动各有优势，要结合系统需求来选择相应的驱动方式。 ​ 对于模型驱动个人有些观点，其实从模型驱动我们可以看出如果采用模型驱动面对一个数据型的应用系统时，最后产生的业务对象模型即退化为了数据模型，只是由于模型驱动通常采用的N层架构此时反而约束了此模型的快速实现，是否应该在模型驱动的N层架构中去考虑一种退化的业务对象模型的支持呢？觉得这点是值得思考的，如果支持的话应该说对于模型驱动非常有利或者说是模型驱动的一个补充，相当于对于模型驱动进行分类处理，有些时候架构不能太S，还是要根据系统建设的需求做出适当的调整。 3.总结 ​ 根据这样的观点，其实数据驱动也是模型驱动，只是它采用的是一种退化的业务对象模型的驱动，并同时进行架构层次的调整以适应系统的快速建设，但数据驱动对于复杂多变的业务逻辑系统来说毕竟难去满足了，主要是会在数据模型的建立以及业务逻辑的修改的方面。 ​ 综合这样的观点，还是更为倾向模型驱动，同时也认为，模型驱动的架构应该考虑对于退化的业务对象模型的支持。]]></content>
      <categories>
        <category>设计与架构</category>
      </categories>
      <tags>
        <tag>设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法--先导篇]]></title>
    <url>%2F2019%2F03%2F07%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%85%88%E5%AF%BC%E7%AF%87%2F</url>
    <content type="text"><![CDATA[数据结构与算法 大学学习的第二个重要的课程就是《数据结构与算法》，当时学下来云里雾里饶，也不知道有什么作用，只是把重要的概念记下来应付考试。工作以后发现除了面试考官经常问起来，更重要的时候工作上很多时候都能用上，用上合适的数据存储结构、作用于特定数据结构上算法后，在空间复杂度和时间复杂度上都有明显的效率提升。为了回顾数据结构与算法并加上印象，特开展专题记录。 1.什么是数据结构与算法? 广义上，也就是从课本上说，数据结构就是一组存储数据的结构,而算法能就是操作数据的一组方法。 狭义上，其实是针对著名的数据结构与算法。著名的意思即前人的智慧结晶，我们时常听到的堆，栈，二分查找，快速排序等都是著名的数据结构与算法。站在巨人的肩膀上，我们能有更卓越的成就。 2.数据结构与算法的关系 总的来说两者是相辅相成的，缺一不可。 数据结构是为算法服务的，算法要作用在特定的数据结构之上。 因此，我们无法孤立数据结构来讲算法，也无法孤立算法来讲数据结构。 数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构就是没用的。 3.学习两者的方向 掌握常用的数据结构和算法的特点（口诀） 重点是学习他们的： “ 来历 ” 、 “ 特点 ” 、 “ 适合解决什么问题 ” 和 “实际的应用场景 ” 。 数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、 Trie 树 算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法 能进行复杂度分析（心法） 效率和资源消耗的度量衡即复杂度分析是数据结构和算法学习的精髓 数据结构和算法解决的是如何更省、更快地存储和处理数据的问题，因此，我们就需要一个考量效率和资源消耗的方法，这就是复杂度分析方法。]]></content>
      <categories>
        <category>算法与模式</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chrome Extension]]></title>
    <url>%2F2019%2F03%2F01%2FChrome-Extension%2F</url>
    <content type="text"><![CDATA[前言使用浏览器扩展程序可以使你的工作效率提高数倍不止，那么下面我就向大家分享一下我日常使用的扩展，可能大多数扩展大家都已经在使用了，不过也难免有一两个是你不知道的。 正文 以下排名并不分先后，请坚持看到最后，或许你会有惊喜。 1.谷歌访问助手本来想了一波广告词来吹它，但想到… 算了，别问，问就是好用。 注：装了这个就可以访问之后介绍的扩展链接了。 链接：http://www.ggfwzs.com/ 2. 新浪微博图床感谢新浪微博提供的免费图床（对外链无限制），以及吊炸天的 cdn 图片加速服务，从此妈妈再也不用担心我的图床不能用了，另外还支持在网页图片右键菜单中一键上传。 链接：https://chrome.google.com/webstore/detail/%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E5%9B%BE%E5%BA%8A/fdfdnfpdplfbbnemmmoklbfjbhecpnhf 3. AdBlock最受欢迎的 Chrome 扩展，拥有超过 6000 万用户！拦截网页上的广告。 链接：https://chrome.google.com/webstore/detail/adblock/gighmmpiobklfepjocnamgkkbiglidom 4. WEB 前端助手前端神器，包括 JSON 格式化、二维码生成与解码、信息编解码、代码压缩、美化、页面取色、Markdown与HTML互转、网页滚动截屏、正则表达式、时间转换工具、编码规范检测、页面性能检测、Ajax接口调试、密码生成器、JSON 比对工具、网页编码设置、便签笔记。 链接：https://chrome.google.com/webstore/detail/pkgccpejnmalmdinmhkkfafefagiiiad 5. JSON Viewer前面介绍的 WEB 前端工具也有 JSON 查看工具，但是它那个太丑了，所以我用这个，内置多种主题，是我见过最好看的 JSON 查看工具。 链接：https://chrome.google.com/webstore/detail/json-viewer/gbmdgpbipfallnflgajpaliibnhdgobh 6. Standardized Screenshot一个非常好用的截图扩展，自动加上 macOS 的标题栏、以及阴影，配合微博图床一键上传根本不用保存在本地。 链接：https://chrome.google.com/webstore/detail/pabdhaakclnechgfhmnhkcbmjobeoope 7. Chromoji - Emoji在某些系统中并不能显示 Emoji 表情，安装此扩展后就能在浏览器中显示和输入，你也能选择显示 Apple 或 Google 风格的 Emoji。 链接：https://chrome.google.com/webstore/detail/chromoji-emoji-for-google/cahedbegdkagmcjfolhdlechbkeaieki 8. Clear Cache如果你是前端开发人员，调试时需要经常清空浏览器缓存，以往我们需要经过几个步骤才能完成动作，现在只需单击一下按钮即可清除缓存和浏览数据。 链接：Clear Cache 9. 二维码生成器把当前页面或者你输入的任何内容转化成二维码，生成后的二维码可以保存，并且无需联网，谁用谁知道。 链接：https://chrome.google.com/webstore/detail/quick-qr-code-generator/afpbjjgbdimpioenaedcjgkaigggcdpp?hl=zh-CN 10. 翻译侠这是我用了众多翻译扩展后最喜欢的一个，貌似之前的作者已经不再维护了，现在由网友接手，挺好的，从此不再需要打开谷歌翻译网站。 链接：https://chrome.google.com/webstore/detail/translate-man/fnjoonbenhhijnoegpfkpagjamomgjjm/related?hl=zh-CN 11. 图流这个厉害了，你是否曾为了看图片，一张一张的点，看完一个系列手都费了，简直痛不欲生，现在，福音来了。 让我们打开知乎问题《平常人可以漂亮到什么程度？》，查看全部答案，开启图流，你会发现。 另外它还支持轮播展示，如果你经常逛一些你懂得网站，那它简直是老司机神器。 链接：https://chrome.google.com/webstore/detail/%E5%9B%BE%E6%B5%81-%E7%9C%8B%E5%9B%BE%E5%8A%A9%E6%89%8B/gpcdnjdgomhddecjpknmfodkpkgibajh?utm_source=chrome-ntp-icon 12. 阅读模式提供与Safari阅读模式功能一致的插件，浏览文章页时候可进入友好的阅读模式，并自定义阅读功能。 唯一美中不足之处就是在阅读模式下无法选中文字（更不能使用划词翻译）。 链接：https://chrome.google.com/webstore/detail/reader-view/iibolhpkjjmoepndefdmdlmbpfhlgjpl 13. Octotree这个估计不少人知道，它可以在 GitHub 左侧显示当前项目的目录结构，能轻松找到代码的位置。 链接：https://chrome.google.com/webstore/detail/octotree/bkhaagjahfmjljalopjnoealnfndnagc 14. Enhanced Github这个可以在 GitHub 中显示仓库大小，每个文件的大小，下载链接和复制文件内容的选项。 要是能够支持单文件夹下载那就更好了。 链接：https://chrome.google.com/webstore/detail/enhanced-github/anlikcnbgdeidpacdbdljnabclhahhmd 15. Isometric Contributions装X神器，值得拥有。这里放上我心中的偶像 Linus 的贡献图。 链接：https://chrome.google.com/webstore/detail/isometric-contributions/mjoedlfflcchnleknnceiplgaeoegien 16. Git History这个扩展可以很炫酷地展示 GitHub 中任意一个文件的历史修改情况。 链接：https://chrome.google.com/webstore/detail/git-history-browser-exten/laghnmifffncfonaoffcndocllegejnf 17. Tampermonkey油猴怕是没有人不知道，它基本上是所有扩展中的佼佼者了。这里给两个可以发现好用的脚本的网站： https://greasyfork.org/zh-CN https://openuserjs.org/ 链接：https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo 18. Stylus其实类似的扩展还有 Stylish，但它会窃取用户的浏览历史（虽然某位药王说我们更愿意用隐私换便利）。 而 Stylus 就不会。 链接：https://chrome.google.com/webstore/detail/stylus/clngdbkpkpeebahjckkjfobafhncgmne ​ 转载来源：https://4ark.me//post/549a6198.html]]></content>
      <categories>
        <category>部署运维</category>
      </categories>
      <tags>
        <tag>browser</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式汇总]]></title>
    <url>%2F2019%2F02%2F26%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[设计模式 设计模式代表了最佳的实践，通常被有经验的面向对象的软件开发人员所采用。设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。 设计模式主要是基于面向对象的以下原则： 面向接口编程而不是实现 推崇对象组合而不是继承 1.设计模式分类共分为三大类 创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 除以上三大类以外还有两类，即并发型模式和线程池模式。 2.设计模式六大原则总原则-开闭原则 对扩展开放，对修改封闭。在程序需要进行拓展的时候，不能去修改原有的代码，而是要扩展原有代码，实现一个热插拔的效果。即：为了使程序的扩展性好，易于维护和升级。 Ⅰ.单一职责原则（Single Responsibility Principle，简称SRP ） 核心思想：应该有且仅有一个原因引起类的变更 问题描述：假如有类Class1完成职责T1，T2，当职责T1或T2有变更需要修改时，有可能影响到该类的另外一个职责正常工作。 好处：类的复杂度降低、可读性提高、可维护性提高、扩展性提高、降低了变更引起的风险。 需注意：单一职责原则提出了一个编写程序的标准，用“职责”或“变化原因”来衡量接口或类设计得是否优良，但是“职责”和“变化原因”都是不可以度量的，因项目和环境而异。 Ⅱ.里氏替换原则（Liskov Substitution Principle,简称LSP） 核心思想：在使用基类的的地方可以任意使用其子类，能保证子类完美替换基类。 通俗来讲：只要父类能出现的地方子类就能出现。反之，父类则未必能胜任。 好处：增强程序的健壮性，即使增加了子类，原有的子类还可以继续运行。 需注意：如果子类不能完整地实现父类的方法，或者父类的某些方法在子类中已经发生“畸变”，则建议断开父子继承关系 采用依赖、聚合、组合等关系代替继承。 Ⅲ.依赖倒置原则（Dependence Inversion Principle,简称DIP） 核心思想：高层模块不应该依赖底层模块，二者都该依赖其抽象；抽象不应该依赖细节；细节应该依赖抽象； 说明：高层模块就是调用端，低层模块就是具体实现类。抽象就是指接口或抽象类。细节就是实现类。 通俗来讲：依赖倒置原则的本质就是通过抽象（接口或抽象类）使个各类或模块的实现彼此独立，互不影响，实现模块间的松耦合。 问题描述：类A直接依赖类B，假如要将类A改为依赖类C，则必须通过修改类A的代码来达成。这种场景下，类A一般是高层模块，负责复杂的业务逻辑；类B和类C是低层模块，负责基本的原子操作；假如修改类A，会给程序带来不必要的风险。 解决方案：将类A修改为依赖接口interface，类B和类C各自实现接口interface，类A通过接口interface间接与类B或者类C发生联系，则会大大降低修改类A的几率。 好处：依赖倒置的好处在小型项目中很难体现出来。但在大中型项目中可以减少需求变化引起的工作量。使并行开发更友好。 Ⅳ.接口隔离原则（Interface Segregation Principle,简称ISP） 核心思想：类间的依赖关系应该建立在最小的接口上 通俗来讲：建立单一接口，不要建立庞大臃肿的接口，尽量细化接口，接口中的方法尽量少。也就是说，我们要为各个类建立专用的接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 问题描述：类A通过接口interface依赖类B，类C通过接口interface依赖类D，如果接口interface对于类A和类B来说不是最小接口，则类B和类D必须去实现他们不需要的方法。 需注意： 接口尽量小，但是要有限度。对接口进行细化可以提高程序设计灵活性，但是如果过小，则会造成接口数量过多，使设计复杂化。所以一定要适度 提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情 为依赖接口的类定制服务。只暴露给调用的类它需要的方法，它不需要的方法则隐藏起来。只有专注地为一个模块提供定制服务，才能建立最小的依赖关系。 Ⅴ.迪米特法则（Law of Demeter,简称LoD） 核心思想：类间解耦。 通俗来讲： 一个类对自己依赖的类知道的越少越好。自从我们接触编程开始，就知道了软件编程的总的原则：低耦合，高内聚。无论是面向过程编程还是面向对象编程，只有使各个模块之间的耦合尽量的低，才能提高代码的复用率。低耦合的优点不言而喻，但是怎么样编程才能做到低耦合呢？那正是迪米特法则要去完成的。 Ⅵ.开放封闭原则（Open Close Principle,简称OCP） 核心思想：尽量通过扩展软件实体来解决需求变化，而不是通过修改已有的代码来完成变化 通俗来讲： 一个软件产品在生命周期内，都会发生变化，既然变化是一个既定的事实，我们就应该在设计的时候尽量适应这些变化，以提高项目的稳定性和灵活性。 总结 单一职责原则告诉我们实现类要职责单一；里氏替换原则告诉我们不要破坏继承体系；依赖倒置原则告诉我们要面向接口编程；接口隔离原则告诉我们在设计接口的时候要精简单一；迪米特法则告诉我们要降低耦合。而开闭原则是总纲，他告诉我们要对扩展开放，对修改关闭。]]></content>
      <categories>
        <category>算法与模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源协议详解]]></title>
    <url>%2F2019%2F02%2F25%2F%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[开源不等于免费！为了加速我们的开发，我们会使用开源的软件和源码； 为避免商业风险，需要在使用时了解第三方如软件协议，版本，和已知CVE风险等；本文旨在从开源软件再发布过程使用权限的角度入手，总结各个常见开源协议的异同，方便理解。 大部分人都希望作品能够被多数人分享查阅。这样不仅提高自己业界的知名度，同时也方便了需要的人为开源做出了贡献。但是代码一旦被贴出来，任何人都可以看到并获取，之后发生的事情你就无法控制了。 所以为了公开分享你的代码，同时又让你对代码保留一定权利，在作品中声明一个许可协议是非常有必要的。有协议和没声明协议的裸代码是有非常重要区别的，一般作品当中没声明协议的默认为Copy right的，也就是版权保留。此种情况表明他人没有任何授权，不得复制分发修改使用等等。有了协议的声明，在未来你的维权上面会方便很多，让你的作品在分享的同时保留了自身的一些权利。 License是软件的授权许可，里面详尽表述了你获得代码后拥有的权利，可以对别人的作品进行何种操作，何种操作又是被禁止的。 软件协议可分为开源和商业 对于商业协议，或者叫法律声明、许可协议，每个软件会有自己的一套行文，由软件作者或专门律师撰写。因为涉及到以后侵权打官司这种事情，这种商业条款的行文是非常严谨而讲究的，读起来很晦涩难懂。 对于开源协议，要知道开源不等于免费，也不等于没有约束。虽然相对商业协议要更加简明，但对于很多人来说还是像在看天书一样。 协议列表 首先一共有哪些公开的协议：https://opensource.org/licenses/alphabetical 常用协议 最流行的六种—-GPL、BSD、MIT、Mozilla、Apache和LGPL。 乌克兰程序员Paul Bagwell，画了一张分析图，说明应该怎么选择，只用两分钟，你就能搞清楚这六种许可证之间的最大区别。 下面是阮一峰中文翻译版本： 1. Apache 许可协议 Apache许可证(Apache License)，是一个在Apache软件基金会发布的自由软件许可证，最初为Apache http服务器而撰写。Apache许可证要求被授权者保留版权和放弃权利的申明，但它不是一个反版权的许可证。 此许可证最新版本为“版本2”，于2004年1月发布。 Apache许可证在Apache社区内外被广泛使用。Apache基金会下属所有项目都使用Apache许可证，许多非Apache基金会项目也使用了Apache许可证：据统计，截至2008年4月，在sourceforge上有超过3000个项目使用了Apache许可证。 Apache 许可协议, 2.0 版本, 授予了用户大量的权利。这些权利可以应用于拷贝权，也可以用于专利权。因为很多许可协议只能适用于拷贝权，不适用于专利权，所以这个灵活性就成了让有专利的开发者们选择许可协议时的一个显著参考因素 (要想明白两者之间的不同，请参考 How Stuff Works 上的这篇文章 )。 下面是关于 Apache 许可协议所允许的事项的详细说明： • 权利永恒。一旦被授权，权利永久不失。 • 权利无疆界。在一个国家里被授权，形同于在所有国家被授权。例如，你在美国，但许可权最初在印度被授予，你同样可以使用这个被授权的程序。 • 授权无需付费和支付酬劳。你既不需要在使用之前支付任何的费用，也无需在每次使用时支付任何的费用，或者其它类似情况。 • 权利不排他。使用这种许可协议下的软件时，不妨碍你使用其它软件。 • 权利不可变更。权利一旦授予，不可剥夺。也就是说，你在使用这个软件的过程中，你无需担心这种情况：当你开发出了令人羡慕的基于这种授权软件的衍生产品时，有人突然跳出来对你说，抱歉，你将不再被允许使用这个程序。 (在这个协议里有个条款声明：如果你控告别人在这个许可协议下的产品有侵犯专利的行为，那你的授权将会自动终止，但这只是适用于有专利权的作品。只要你不搞有专利作品的诉讼，你永远无需担心这种问题。) • 对再分发的作品还有个特殊要求，总的就是说要给予这些程序的作者和许可协议的维护者适当的名誉。 2. MIT 许可协议 MIT 协议应该是在流行的开源协议中最简短的、使用最广泛的一种协议。它的条款非常的宽松，而且跟其它协议相比更自由。 MIT 协议是目前最少限制的协议。 它基本上就是任何人可以对这个协议下的软件的做任何的事情，只要你能认可这个协议。这种协议最基本的条款 ( the information that it is provided without warranty, which comprises the final paragraph)如下： 特此授权，任何人都可免费获得这个软件以及相关文档（the Software）的拷贝，可以无限制的使用这个软件，包括无限制的权利去使用、复制、修改、合并、发布、附加从属协议，以及/或者出售软件的拷贝， 同时，为了让软件的提供者有权利做到这些，下面的条件必须遵守： 上面的拷贝权声明和许可声明必须包含在所有的这个软件拷贝里和实际分署部分里。 这也就是说： • 你可以随意使用，复制，修改这个软件。没有人能够阻止你在任何工程里使用它，你可以复制任意次数、以任何形式，或按你的愿望修改它。 • 你可以向外免费发放，或出售。你可以随意的分发它，没有任何限制。 • 唯一的限制是你必须接受协议条款。 3. BSD 许可协议 BSD 协议有很多分支，它们都代表了一种宽松的自由软件协议，相对其它协议，例如GPL，来说，它们对软件的传播给予了更少的限制。 在这种协议的各种版本中，有两个版本格外的重要： 新 BSD 协议/修订版 BSD 协议和简化 BSD 协议/FreeBSD 协议。这两类协议都实现的对 GPL 兼容的自由软件协议，而且被 Open Source Initiative 认可为开源软件协议。 新 BSD 协议(3-clause license)无任何限制的允许你以任何目的二次分发这种软件，唯一的要求是必须保留拷贝权的声明和协议里的软件权利放弃条款。这种协议还有一个限制，未经许可不得使用这个作品的所有曾经捐助者的署名。 新 BSD 协议和简化 BSD 协议的最主要的区别是后者删除了署名条款。 BSD开源协议是一个给于使用者很大自由的协议。基本上使用者可以”为所欲为”，可以自由的使用，修改源代码，也可以将修改后的代码作为开源或者专有软件再发布。 但”为所欲为”的前提当你发布使用了BSD协议的代码，或则以BSD协议代码为基础做二次开发自己的产品时，需要满足三个条件： • 如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中的BSD协议。 • 如果再发布的只是二进制类库/软件，则需要在类库/软件的文档和版权声明中包含原来代码中的BSD协议。 • 不可以用开源代码的作者/机构名字和原来产品的名字做市场推广。 • BSD 代码鼓励代码共享，但需要尊重代码作者的著作权。BSD由于允许使用者修改和重新发布代码，也允许使用或在BSD代码上开发商业软件发布和销售，因此是对商业集成很友好的协议。而很多的公司企业在选用开源产品的时候都首选BSD协议，因为可以完全控制这些第三方的代码，在必要的时候可以修改或者二次开发。 4. GPL许可协议 我们很熟悉的Linux就是采用了GPL。GPL协议和BSD， Apache Licence等鼓励代码重用的许可很不一样。GPL的出发点是代码的开源/免费使用和引用/修改/衍生代码的开源/免费使用，但不允许修改后和衍生的代 码做为闭源的商业软件发布和销售。 这也就是为什么我们能用免费的各种linux，包括商业公司的linux和linux上各种各样的由个人，组织，以及商 业软件公司开发的免费软件了。 GPL协议的主要内容是只要在一个软件中使用（”使用”指类库引用，修改后的代码或者衍生代码）GPL 协议的产品，则该软件产品必须也采用GPL协议，既必须也是开源和免费。这就是所谓的”传染性”。GPL协议的产品作为一个单独的产品使用没有任何问题， 还可以享受免费的优势。 由于GPL严格要求使用了GPL类库的软件产品必须使用GPL协议，对于使用GPL协议的开源代码，商业软件或者对代码有保密要求的部门就不适合集成/采用作为类库和二次开发的基础。 其它细节如再发布的时候需要伴随GPL协议等和BSD/Apache等类似。 5. LGPL许可协议 LGPL 是GPL的一个为主要为类库使用设计的开源协议。和GPL要求任何使用/修改/衍生之GPL类库的的软件必须采用GPL协议不同。LGPL 允许商业软件通过类库引用（link）方式使用LGPL类库而不需要开源商业软件的代码。这使得采用LGPL协议的开源代码可以被商业软件作为类库引用并 发布和销售。 但是如果修改LGPL协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用LGPL协议。因 此LGPL协议的开源 代码很适合作为第三方类库被商业软件引用，但不适合希望以LGPL协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。 GPL/LGPL都保障原作者的知识产权，避免有人利用开源代码复制并开发类似的产品。 6. MPL许可协议 MPL是The Mozilla Public License的简写，是1998年初Netscape的 Mozilla小组为其开源软件项目设计的软件许可证。 MPL许可证出现的最重要原因就是，Netscape公司认为GPL许可证没有很好地平衡开发者对源代码的需求和他们利用源代码获得的利益。同著名的GPL许可证和BSD许可证相比，MPL在许多权利与义务的约定方面与它们相同（因为都是符合OSIA认定的开源软件许可证）。 但是，相比而言MPL还有以下几个显著的不同之处： - MPL虽然要求对于经MPL许可证发布的源代码的修改也要以MPL许可证的方式再许可出来，以保证其他人可以在MPL的条款下共享源代码。但是，在MPL许可证中对“发布”的定义是“以源代码方式发布的文件”，这就意味着MPL允许一个企业在自己已有的源代码库上加一个接口，除了接口程序的源代码以MPL许可证的形式对外许可外，源代码库中的源代码就可以不用MPL许可证的方式强制对外许可。这些，就为借鉴别人的源代码用做自己商业软件开发的行为留了一个豁口。 - MPL许可证第三条第7款中允许被许可人将经过MPL许可证获得的源代码同自己其他类型的代码混合得到自己的软件程序。 对软件专利的态度，MPL许可证不像GPL许可证那样明确表示反对软件专利，但是却明确要求源代码的提供者不能提供已经受专利保护的源代码（除非他本人是专利权人，并书面向公众免费许可这些源代码），也不能在将这些源代码以开放源代码许可证形式许可后再去申请与这些源代码有关的专利。 对源代码的定义 • 而在MPL（1.1版本）许可证中，对源代码的定义是:“源代码指的是对作品进行修改最优先择取的形式，它包括:所有模块的所有源程序，加上有关的接口的定义，加上控制可执行作品的安装和编译的‘原本’（原文为‘Script’），或者不是与初始源代码显著不同的源代码就是被源代码贡献者选择的从公共领域可以得到的程序代码。” • MPL许可证第3条有专门的一款是关于对源代码修改进行描述的规定，就是要求所有再发布者都得有一个专门的文件就对源代码程序修改的时间和修改的方式有描述。 小结 GPL协议、LGPL协议与BSD协议的法律区别。 简而言之，GPL协议就是一个开放源代码协议，软件的初始开发者使用了GPL协议并公开软件的源程序后，后续使用该软件源程序开发软件者亦应当根据GPL协议把自己编写的源程序进行公开。GPL协议要求的关键在于开放源程序，但并不排斥软件作者向用户收费。 虽然如此，很多大公司对GPL协议还是又爱又恨，爱的是这个协议项下的软件历经众多程序员千锤百炼的修改，已经非常成熟完善，恨的是必须开放自己后续的源程序，导致竞争对手也可以根据自己修改的源程序开发竞争产品。 正因大公司对GPL协议在商业上存在顾虑，因此，另两种协议被采用的更多，第一种是LGPL（亦称GPL V2）协议，可以翻译为更宽松的GPL协议。与GPL协议的区别为，后者如果只是对LGPL软件的程序库的程序进行调用而不是包含其源代码时，相关的源程序无需开源。 调用和包含的区别类似在互联网网网页上对他人网页内容的引用：如果把他人的内容全部或部分复制到自己的网页上，就类似包含，如果只是贴一个他人网页的网址链接而不引用内容，就类似调用。有了这个协议，很多大公司就可以把很多自己后续开发内容的源程序隐藏起来。 第二种是BSD协议（类似的还有MIT协议）。BSD协议鼓励软件的作者公开自己后续开发的源代码，但不强求。在BSD协议项下开发的软件，原始的源程序是开放源代码的，但使用者修改以后，可以自行选择发布源程序或者二进制程序（即目标程序），当然，使用者有义务把自己原来使用的源程序与BSD协议在软件对外发布时一并发布。因为比较灵活，所以BSD深受大公司的欢迎。]]></content>
      <categories>
        <category>技术杂记</category>
      </categories>
      <tags>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Map--Usage]]></title>
    <url>%2F2019%2F01%2F31%2FMap%2F</url>
    <content type="text"><![CDATA[Map1. putIfAbsent和computeIfAbsent 都是在key不存在的时候才会建立key和value的映射关系；putIfAbset不论传入的value是否为空，都会建立映射（并不适合所有子类，例如HashTable），而computeIfAbsent方法，当存入value为空时，不做任何操作当key不存在时，返回的都是新的value（为什么不说新插入的value），即使computeIfAbsent在传入的value为null时，不会新建映射关系，但返回的也是null； 2. computeIfPresent和computeIfAbsent 这两个方法正好相反，前者是在key存在时，才会用新的value替换oldValue当传入的key存在，并且传入的value为null时，前者会remove（key），把传入的key对应的映射关系移除；而后者不论何时都不会remove()；前者只有在key存在，并且传入的value不为空的时候，返回值是value，其他情况都是返回null；后者只有在key不存在，并且传入的value不为null的时候才会返回value，其他情况都返回null； 3. compute 新传入的value不为null就建立映射关系（也就是说不论key是否为null，具体子类再具体分析）新传入的value为null时：key已存在，且老的对应value不为null，移除改映射关系，返回null；否则，直接返回null]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Drools--Group]]></title>
    <url>%2F2018%2F09%2F05%2FDrools%2F</url>
    <content type="text"><![CDATA[Drools-Group123456789101112131415/***具有相同activation-group属性的规则中只要有一个被执行，其他的规则都不再执行。*可以用类似salience之类的属性来实现规则的执行优先级*/activation-group/***将规则划分一个的组，然后在规则流当中通过使用ruleflow-group属性的值，从而使用*对应的规则，该属性会通过流程的走向确定要执行哪一条规则*/ruleflow-group/***Agenda Group 是用来在Agenda基础上对规则进行再次分组。引擎在调用设置了agenda-group*属性的规则时需要显示的指定某个agenda group 获得focus，否则将不执行此组的规则。*/agenda-group 1. Agenda-group 议程组允许将规则以组的形式，并将这么组放入堆栈，堆栈具有push/pop操作，通过SetFocus完成Push操作。1kieSession.getAgenda().getAgendaGroup("group2").setFocus(); 议程总是在栈顶执行，当议程组所有的规则执行后，这个议程组会被移除然后执行下一组。1234567891011121314151617181920212223242526public void excuteAgendaGroup()&#123; KieBase kieBase = KieStarter.newKieBase(getAgendaGroupDrl()); KieSession kieSession = kieBase.newKieSession(); //group2 后执行 kieSession.getAgenda().getAgendaGroup("group2").setFocus(); //group1 先执行 kieSession.getAgenda().getAgendaGroup("group1").setFocus(); kieSession.fireAllRules(); kieSession.dispose();&#125;private String getAgendaGroupDrl()&#123; return "rule \"test-agenda-group1\" " + "agenda-group \"group1\" " + "when then System.out.println(\"test-agenda-group1-1 被触发\"); " + "end " + "rule \"test-agenda-group2\" " + "agenda-group \"group2\" " + "when " + "then " + "System.out.println(\"test-agenda-group2 被触发\"); end\n" + "rule \"test-agenda-group3\" " + "agenda-group \"group1\" " + "when then System.out.println(\"test-agenda-group1-2 被触发\"); " + "end ";&#125; 2.Activation-Group 激活组要求相同组只执行一个123456789101112131415161718public void executeActivationGroup()&#123; KieBase kieBase = KieStarter.newKieBase(getActivationGroupDrl()); KieSession kieSession = kieBase.newKieSession(); kieSession.fireAllRules();&#125;private String getActivationGroupDrl()&#123; return "rule \"test-activation-group1\" " + "salience 1"+ "activation-group \"group1\" " + "when then System.out.println(\"test-activation-group1 被触发\"); " + "end " + "rule \"test-activation-group2\" " + "salience 2"+ "activation-group \"group1\" " + "when " + "then " + "System.out.println(\"test-activation-group2 被触发\"); end\n";&#125; 3. Flow-Group 流式组支持服务编排，像工作流一样先后顺序运行规则]]></content>
      <categories>
        <category>技术杂记</category>
      </categories>
      <tags>
        <tag>规则引擎</tag>
      </tags>
  </entry>
</search>
